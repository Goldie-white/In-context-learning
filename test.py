#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•Yé¢„æµ‹å™¨ï¼šåˆ†æå„ä¸ªä½ç½®çš„yé¢„æµ‹loss
"""

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import json
import pickle
import numpy as np
from absl import app, flags, logging
from flax import jax_utils
from flax.training import checkpoints
from jax import random
import jax
import jax.numpy as jnp

from incontext import utils
from incontext import sampler_lib
from incontext import transformer_lib_flax
from incontext import predictor_flax

flags.DEFINE_string("checkpoint_dir", default="experiments/y_predictor/ckpt", help="æ£€æŸ¥ç‚¹ç›®å½•")
flags.DEFINE_integer("n_test_samples", default=500, help="æµ‹è¯•æ ·æœ¬æ•°")
flags.DEFINE_integer("seed", default=42, help="æµ‹è¯•éšæœºç§å­")
flags.DEFINE_string("output_file", default="experiments/y_predictor/y_analysis.pkl", help="è¾“å‡ºæ–‡ä»¶")
flags.DEFINE_string("test_x_distribution_str", default=None, help="æµ‹è¯•æ—¶xåˆ†å¸ƒï¼ˆNoneåˆ™ä½¿ç”¨è®­ç»ƒåˆ†å¸ƒï¼‰")
flags.DEFINE_string("test_w_distribution_str", default=None, help="æµ‹è¯•æ—¶wåˆ†å¸ƒï¼ˆNoneåˆ™ä½¿ç”¨è®­ç»ƒåˆ†å¸ƒï¼‰")
flags.DEFINE_float("test_prob0", default=None, help="æµ‹è¯•æ—¶ä»»åŠ¡1æ¦‚ç‡ï¼ˆNoneåˆ™ä½¿ç”¨è®­ç»ƒè®¾ç½®ï¼‰")
flags.DEFINE_float("test_prob1", default=None, help="æµ‹è¯•æ—¶ä»»åŠ¡2æ¦‚ç‡ï¼ˆNoneåˆ™ä½¿ç”¨è®­ç»ƒè®¾ç½®ï¼‰")
flags.DEFINE_float("test_prob2", default=None, help="æµ‹è¯•æ—¶ä»»åŠ¡3æ¦‚ç‡ï¼ˆNoneåˆ™ä½¿ç”¨è®­ç»ƒè®¾ç½®ï¼‰")
flags.DEFINE_float("test_prob3", default=None, help="æµ‹è¯•æ—¶ä»»åŠ¡4æ¦‚ç‡ï¼ˆNoneåˆ™ä½¿ç”¨è®­ç»ƒè®¾ç½®ï¼‰")
flags.DEFINE_float("test_task_mix_alpha", default=None, help="[å·²å¼ƒç”¨] æµ‹è¯•æ—¶ä»»åŠ¡æ··åˆæ¯”ä¾‹ï¼ˆNoneåˆ™ä½¿ç”¨è®­ç»ƒè®¾ç½®ï¼‰")
flags.DEFINE_float("test_task3_prob", default=None, help="[å·²å¼ƒç”¨] æµ‹è¯•æ—¶ä»»åŠ¡3æ¦‚ç‡ï¼ˆNoneåˆ™ä½¿ç”¨è®­ç»ƒè®¾ç½®ï¼‰")

FLAGS = flags.FLAGS


def load_model_and_config(checkpoint_dir):
    """åŠ è½½æ¨¡å‹å’Œé…ç½®"""
    # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    checkpoint_dir = os.path.abspath(checkpoint_dir)
    
    # åŠ è½½é…ç½®
    config_path = os.path.join(os.path.dirname(checkpoint_dir), "config.json")
    with open(config_path, 'r') as f:
        config_dict = json.load(f)
    
    args = utils.dict_to_args(config_dict)
    
    # åˆ›å»ºTransformeré…ç½®
    config = transformer_lib_flax.TransformerConfig(
        num_heads=getattr(args, 'n_heads', 4),
        num_layers=getattr(args, 'n_layers', 16),
        hidden_size=getattr(args, 'hidden_size', 512),
        loss_on_x_steps=args.loss_on_x_steps,
        norm_first=args.norm_first,
        disable_layer_norms=args.disable_layer_norms,
        final_layer_norm=args.final_layer_norm,
        kernel_init=transformer_lib_flax.nn_init_parser(args.kernel_init),
        bias_init=transformer_lib_flax.nn_init_parser(args.bias_init),
        linear_w_init=transformer_lib_flax.nn_init_parser(args.linear_w_init),
        linear_bias_init=transformer_lib_flax.nn_init_parser(args.linear_bias_init),
        posemb_init=transformer_lib_flax.nn_init_parser(args.posemb_init),
        max_len=(args.num_exemplars + 1) * 2,
        inner_dim=None,
        activation_fn=transformer_lib_flax.nn_activation_parser(args.activation_fn),
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = predictor_flax.CausalLM(config)
    
    # åˆå§‹åŒ–å˜é‡
    rng = random.PRNGKey(0)
    init_batch = jnp.ones((1, config.max_len, args.x_dim + 1), jnp.float32)
    init_variables = model.init(rng, inputs=init_batch, train=False)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    restored = checkpoints.restore_checkpoint(checkpoint_dir, target=None)
    params = restored['params']
    
    logging.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {checkpoint_dir}")
    logging.info(f"   é…ç½®: L={config.num_layers}, H={config.hidden_size}, M={config.num_heads}")
    logging.info(f"   æ•°æ®: {args.num_exemplars}ä¸ªæ ·æœ¬å¯¹, x_dim={args.x_dim}")
    
    # æ˜¾ç¤ºåˆ†å¸ƒé…ç½®
    x_dist = getattr(args, 'x_distribution_str', 'N/A')
    w_dist = getattr(args, 'w_distribution_str', 'N/A')
    logging.info(f"   åˆ†å¸ƒ: p(x)={x_dist}, p(w)={w_dist}")
    
    return model, params, args


def test_y_predictor(args):
    """æµ‹è¯•Yé¢„æµ‹å™¨"""
    # è®¾ç½®éšæœºç§å­
    utils.set_seed(args.seed)
    rng = random.PRNGKey(args.seed)
    
    # åŠ è½½æ¨¡å‹
    model, params, train_args = load_model_and_config(args.checkpoint_dir)
    
    # ç¡®å®šæµ‹è¯•ä½¿ç”¨çš„åˆ†å¸ƒï¼ˆä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨è®­ç»ƒåˆ†å¸ƒï¼‰
    test_x_dist_str = args.test_x_distribution_str if args.test_x_distribution_str else train_args.x_distribution_str
    test_w_dist_str = args.test_w_distribution_str if args.test_w_distribution_str else train_args.w_distribution_str
    
    # ç¡®å®šä»»åŠ¡æ¦‚ç‡ï¼ˆä¼˜å…ˆä½¿ç”¨æµ‹è¯•æŒ‡å®šçš„å€¼ï¼Œå¦åˆ™ä½¿ç”¨è®­ç»ƒè®¾ç½®ï¼‰
    # Check if any test_prob is specified
    if any(p is not None for p in [args.test_prob0, args.test_prob1, args.test_prob2, args.test_prob3]):
        # Use test probabilities (default to training values for unspecified)
        test_prob0 = args.test_prob0 if args.test_prob0 is not None else getattr(train_args, 'prob0', 1.0)
        test_prob1 = args.test_prob1 if args.test_prob1 is not None else getattr(train_args, 'prob1', 0.0)
        test_prob2 = args.test_prob2 if args.test_prob2 is not None else getattr(train_args, 'prob2', 0.0)
        test_prob3 = args.test_prob3 if args.test_prob3 is not None else getattr(train_args, 'prob3', 0.0)
        
        task_probs = [test_prob0, test_prob1, test_prob2, test_prob3]
        prob_sum = sum(task_probs)
        
        # Validate probabilities sum to 1.0
        if abs(prob_sum - 1.0) > 1e-6:
            raise ValueError(
                f"æµ‹è¯•ä»»åŠ¡æ¦‚ç‡ä¹‹å’Œå¿…é¡»ç­‰äº1.0ï¼Œå½“å‰ä¸º {prob_sum}ã€‚\n"
                f"å½“å‰è®¾ç½®: test_prob0={test_prob0}, test_prob1={test_prob1}, "
                f"test_prob2={test_prob2}, test_prob3={test_prob3}\n"
                f"è¯·è°ƒæ•´å‚æ•°ä½¿å…¶å’Œä¸º1.0"
            )
        
        logging.info(f"ğŸ“ ä½¿ç”¨æµ‹è¯•ä»»åŠ¡æ¦‚ç‡: [Task1={test_prob0}, Task2={test_prob1}, Task3={test_prob2}, Task4={test_prob3}]")
    else:
        # Use training probabilities
        train_prob0 = getattr(train_args, 'prob0', 1.0)
        train_prob1 = getattr(train_args, 'prob1', 0.0)
        train_prob2 = getattr(train_args, 'prob2', 0.0)
        train_prob3 = getattr(train_args, 'prob3', 0.0)
        task_probs = [train_prob0, train_prob1, train_prob2, train_prob3]
        logging.info(f"ğŸ“ ä½¿ç”¨è®­ç»ƒæ—¶çš„ä»»åŠ¡æ¦‚ç‡: [Task1={train_prob0}, Task2={train_prob1}, Task3={train_prob2}, Task4={train_prob3}]")
    
    # Create sampler
    sampler = sampler_lib.Sampler(
        train_args.num_exemplars,
        train_args.x_dim,
        train_args.hidden_size,
        x_distribution_fn=sampler_lib.str_to_distribution_fn(test_x_dist_str),
        w_distribution_fn=sampler_lib.str_to_distribution_fn(test_w_dist_str),
        noise_std=train_args.noise_std,
        task_probs=task_probs,
    )
    
    logging.info(f"ğŸ§ª å¼€å§‹æµ‹è¯•ï¼Œç”Ÿæˆ {args.n_test_samples} ä¸ªæµ‹è¯•æ ·æœ¬...")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    seqs, coefficients, xs, ys = sampler.sample(n=args.n_test_samples)
    seqs = jnp.array(seqs)
    ys_true = np.array(ys)  # (n_samples, num_exemplars, 1) - ä¿å­˜çœŸå®yå€¼
    
    # å‰å‘ä¼ æ’­
    logging.info("ğŸ“Š è®¡ç®—å„ä½ç½®çš„yé¢„æµ‹loss...")
    errors, (y_errors, y_pred, seq_pred, seq_hiddens) = model.apply(
        {"params": params},
        inputs=seqs,
        train=False,
        return_attention=False
    )
    
    # è½¬ä¸ºnumpy
    y_errors = np.array(y_errors)
    y_pred = np.array(y_pred)  # (n_samples, num_exemplars, 1)
    
    # è®¡ç®—å¹³å‡loss
    avg_y_loss = np.mean(y_errors, axis=0)  # (num_exemplars,)
    
    # è®¡ç®—å„ä½ç½®yé¢„æµ‹çš„å‡å€¼å’Œæ ‡å‡†å·®
    y_mean_per_pos = np.mean(y_pred, axis=0)  # (num_exemplars, 1)
    y_std_per_pos = np.std(y_pred, axis=0)    # (num_exemplars, 1)
    
    # è®¡ç®—å„ä½ç½®yçœŸå®å€¼çš„å‡å€¼å’Œæ ‡å‡†å·®
    y_true_mean_per_pos = np.mean(ys_true, axis=0)  # (num_exemplars, 1)
    y_true_std_per_pos = np.std(ys_true, axis=0)    # (num_exemplars, 1)
    
    # è¾“å‡ºç»“æœ
    logging.info("\n" + "="*70)
    logging.info("æµ‹è¯•ç»“æœåˆ†æ")
    logging.info(f"å®éªŒ: {train_args.exp_folder if hasattr(train_args, 'exp_folder') else args.checkpoint_dir}")
    train_x_dist = getattr(train_args, 'x_distribution_str', 'N/A')
    train_w_dist = getattr(train_args, 'w_distribution_str', 'N/A')
    logging.info(f"è®­ç»ƒåˆ†å¸ƒ: p(x)={train_x_dist}, p(w)={train_w_dist}")
    logging.info(f"æµ‹è¯•åˆ†å¸ƒ: p(x)={test_x_dist_str}, p(w)={test_w_dist_str}")
    if test_x_dist_str != train_x_dist or test_w_dist_str != train_w_dist:
        logging.info("âš ï¸  æ³¨æ„: æµ‹è¯•åˆ†å¸ƒä¸è®­ç»ƒåˆ†å¸ƒä¸åŒ (Out-of-Distribution æµ‹è¯•)")
    logging.info("="*70)
    
    # è¾“å‡ºå„ä½ç½®çš„lossï¼ˆå’Œè®­ç»ƒæ—¶æ ¼å¼ä¸€è‡´ï¼‰
    loss_str = "[" + ", ".join([f"{float(avg_y_loss[i]):.4f}" for i in range(len(avg_y_loss))]) + "]"
    logging.info(f"\nå„ä½ç½®Yé¢„æµ‹Loss: {loss_str}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    logging.info("\n" + "="*70)
    logging.info("ç»Ÿè®¡ä¿¡æ¯")
    logging.info("="*70)
    logging.info(f"å¹³å‡Loss: {np.mean(avg_y_loss):.6f}")
    logging.info(f"æœ€å°Lossï¼ˆä½ç½®{np.argmin(avg_y_loss)}ï¼‰: {np.min(avg_y_loss):.6f}")
    logging.info(f"æœ€å¤§Lossï¼ˆä½ç½®{np.argmax(avg_y_loss)}ï¼‰: {np.max(avg_y_loss):.6f}")
    logging.info(f"ç¬¬1ä¸ªä½ç½®Loss: {avg_y_loss[0]:.6f}")
    logging.info(f"æœ€åä½ç½®Loss: {avg_y_loss[-1]:.6f}")
    logging.info(f"Lossä¸‹é™: {avg_y_loss[0] - avg_y_loss[-1]:.6f} ({(1 - avg_y_loss[-1]/avg_y_loss[0])*100:.1f}%)")
    
    # ä¿å­˜ç»“æœï¼ˆåŒ…å«åˆ†å¸ƒä¿¡æ¯ï¼‰
    os.makedirs(os.path.dirname(args.output_file), exist_ok=True)
    analysis = {
        'avg_y_loss': avg_y_loss,
        'y_errors': y_errors,  # æ‰€æœ‰æ ·æœ¬çš„loss
        'y_mean_per_pos': y_mean_per_pos,  # å„ä½ç½®yé¢„æµ‹çš„å‡å€¼
        'y_std_per_pos': y_std_per_pos,    # å„ä½ç½®yé¢„æµ‹çš„æ ‡å‡†å·®
        'y_true_mean_per_pos': y_true_mean_per_pos,  # å„ä½ç½®yçœŸå®å€¼çš„å‡å€¼
        'y_true_std_per_pos': y_true_std_per_pos,    # å„ä½ç½®yçœŸå®å€¼çš„æ ‡å‡†å·®
        'train_x_distribution_str': train_x_dist,
        'train_w_distribution_str': train_w_dist,
        'test_x_distribution_str': test_x_dist_str,
        'test_w_distribution_str': test_w_dist_str,
        'exp_folder': getattr(train_args, 'exp_folder', 'N/A'),
    }
    with open(args.output_file, 'wb') as f:
        pickle.dump(analysis, f)
    
    logging.info(f"\nâœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {args.output_file}")
    logging.info("="*70)
    
    return analysis


def main(_):
    """ä¸»å‡½æ•°"""
    args = utils.flags_to_args()
    test_y_predictor(args)


if __name__ == "__main__":
    app.run(main)

