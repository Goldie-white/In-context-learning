#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸Šä¸‹æ–‡å­¦ä¹ è®­ç»ƒè„šæœ¬ - Wé¢„æµ‹ç‰ˆæœ¬
é¢„æµ‹æƒé‡å‘é‡wè€Œä¸æ˜¯ç›´æ¥é¢„æµ‹yï¼Œç„¶åé€šè¿‡y=w^Txè®¡ç®—yå€¼
"""

import os
# å¼ºåˆ¶ä½¿ç”¨å•GPUé¿å…NCCLå¤šGPUé—®é¢˜
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import json
import pickle
import numpy as np
from absl import app, flags, logging
from flax import jax_utils
from flax.training import train_state
from flax.training import common_utils
from flax.training import checkpoints
from jax import random
import jax
import jax.numpy as jnp
import optax
from tensorflow.io import gfile

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from incontext import utils
from incontext import sampler_lib
from incontext import transformer_lib_flax
from incontext import predictor_flax_w_loss_w  # â­ æ”¹åŠ¨1ï¼šå¯¼å…¥wé¢„æµ‹å™¨ï¼ˆä½¿ç”¨w lossç‰ˆæœ¬ï¼‰

# åŸºç¡€è®­ç»ƒå‚æ•° (ä¸åŸå§‹é¡¹ç›®model_trainer.pyå’Œmain.pyä¸€è‡´)
flags.DEFINE_integer("seed", default=0, help="éšæœºç§å­")
flags.DEFINE_integer("batch_size", default=64, help="æ‰¹æ¬¡å¤§å°")
flags.DEFINE_integer("x_dim", default=20, help="è¾“å…¥ç»´åº¦")
flags.DEFINE_integer("num_exemplars", default=None, help="ç¤ºä¾‹æ•°é‡ï¼ˆå›ºå®šé•¿åº¦æ¨¡å¼ï¼Œè®¾ç½®æ­¤é¡¹åˆ™å¿½ç•¥min/maxï¼‰")
flags.DEFINE_integer("min_num_exemplars", default=20, help="æœ€å°ç¤ºä¾‹æ•°é‡ï¼ˆå¯å˜é•¿åº¦æ¨¡å¼ï¼‰")
flags.DEFINE_integer("max_num_exemplars", default=50, help="æœ€å¤§ç¤ºä¾‹æ•°é‡ï¼ˆå¯å˜é•¿åº¦æ¨¡å¼ï¼‰")
flags.DEFINE_integer("n_epochs", default=5000, help="è®­ç»ƒè½®æ•°")  # åŸå§‹é¡¹ç›®: 5001 epochs
flags.DEFINE_integer("n_iter_per_epoch", default=100, help="æ¯è½®è¿­ä»£æ¬¡æ•°")  # åŸå§‹é¡¹ç›®: 100 iters, æ€»å…±~500Kæ­¥
flags.DEFINE_float("learning_rate", default=1e-4, help="å­¦ä¹ ç‡")  # åŸå§‹é¡¹ç›®: 1e-4
flags.DEFINE_float("weight_decay", default=0, help="æƒé‡è¡°å‡")  # åŸå§‹é¡¹ç›®: 0
flags.DEFINE_string("exp_folder", default="experiments/w_predictor", help="å®éªŒæ–‡ä»¶å¤¹")  # â­ æ”¹åŠ¨2ï¼šç‹¬ç«‹æ–‡ä»¶å¤¹é¿å…æ··æ·†

# æ•°æ®åˆ†å¸ƒå‚æ•°
flags.DEFINE_string("x_distribution_str", default="normal*1.0+0.0", help="è¾“å…¥åˆ†å¸ƒ")
flags.DEFINE_string("w_distribution_str", default="normal*1.0+0.0", help="æƒé‡åˆ†å¸ƒ")
flags.DEFINE_float("noise_std", default=0.0, help="å™ªå£°æ ‡å‡†å·®")
flags.DEFINE_float("prob0", default=1.0, help="ä»»åŠ¡1æ¦‚ç‡: y=w^Tx (æ ‡å‡†çº¿æ€§å›å½’)")
flags.DEFINE_float("prob1", default=0.0, help="ä»»åŠ¡2æ¦‚ç‡: y=w^Tsort(x) (æ’åºçº¿æ€§å›å½’)")
flags.DEFINE_float("prob2", default=0.0, help="ä»»åŠ¡3æ¦‚ç‡: y=(dim/sqrt(2))*w^Tsoftmax(x) (ç¼©æ”¾softmaxçº¿æ€§å›å½’)")
flags.DEFINE_float("prob3", default=0.0, help="ä»»åŠ¡4æ¦‚ç‡: y=||x-w||^2 (å¹³æ–¹è·ç¦»)")
flags.DEFINE_float("task_mix_alpha", default=1.0, help="[å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨prob0-prob3] ä»»åŠ¡æ··åˆæ¯”ä¾‹")
flags.DEFINE_float("task3_prob", default=0.0, help="[å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨prob0-prob3] ä»»åŠ¡3æ¦‚ç‡")

# Transformeræ¨¡å‹å‚æ•° (åœ¨ transformer_lib_flax.py ä¸­å®šä¹‰)
flags.DEFINE_float("dropout_rate", default=0.0, help="Dropout rate (0.0-1.0), å»ºè®®å€¼: 0.1-0.2")
flags.DEFINE_float("attention_dropout_rate", default=0.0, help="Attention dropout rate (0.0-1.0), å»ºè®®å€¼: 0.1-0.2")

# ä¼˜åŒ–å™¨å‚æ•°
flags.DEFINE_string("lr_scheduler_type", default="cosine", help="å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹")
flags.DEFINE_float("adam_b1", default=0.9, help="Adam b1")
flags.DEFINE_float("adam_b2", default=0.98, help="Adam b2")
flags.DEFINE_float("adam_eps", default=1e-9, help="Adam eps")

FLAGS = flags.FLAGS


def train_step(state, seq, task_ids, w_target, model, learning_rate_fn, dropout_rng=None):
    """æ‰§è¡Œå•æ­¥è®­ç»ƒ"""
    dropout_rng = random.fold_in(dropout_rng, state.step)

    def loss_fn(params):
        """è®­ç»ƒæŸå¤±å‡½æ•° - ä½¿ç”¨ w çš„ MSE"""
        output = model.apply({"params": params},
                           inputs=seq,
                           task_ids=task_ids,
                           w_target=w_target,  # â­ ä¼ å…¥çœŸå®çš„wå‘é‡
                           train=True,
                           rngs={"dropout": dropout_rng})
        return output[0].mean(), output

    lr = learning_rate_fn(state.step)
    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
    (_, extras), grads = grad_fn(state.params)
    grads = jax.lax.pmean(grads, "batch")
    new_state = state.apply_gradients(grads=grads)
    loss = jax.lax.pmean(extras[0], "batch")
    # extras[1] = (y_errors, w_errors, y_pred, w_pred, seq_pred, seq_hiddens)
    y_errors = jax.lax.psum(extras[1][0], "batch").sum(axis=0)
    w_errors = jax.lax.psum(extras[1][1], "batch").sum(axis=0)
    metrics = {"loss": loss, "lr": lr, "y_errors": y_errors, "w_errors": w_errors}
    return new_state, metrics


def get_model(rng, args):
    """åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨"""
    rng, init_rng = random.split(rng)

    # è®ºæ–‡æ ‡å‡†é…ç½®ï¼šL=16, H=512, M=4
    # å¦‚æœargsä¸­æ²¡æœ‰è®¾ç½®ï¼Œä½¿ç”¨è®ºæ–‡æ ‡å‡†å€¼
    n_layers = getattr(args, 'n_layers', 16)
    hidden_size = getattr(args, 'hidden_size', 512)
    n_heads = getattr(args, 'n_heads', 4)
    
    # ç¡®å®šæœ€å¤§é•¿åº¦ï¼šå¦‚æœä½¿ç”¨å¯å˜é•¿åº¦ï¼Œä½¿ç”¨max_num_exemplarsï¼›å¦åˆ™ä½¿ç”¨num_exemplars
    if args.num_exemplars is not None:
        max_num_exemplars = args.num_exemplars
    else:
        max_num_exemplars = args.max_num_exemplars
    
    logging.info(f"Transformeré…ç½®: L={n_layers}, H={hidden_size}, M={n_heads}")
    logging.info(f"â­ ä½¿ç”¨Wé¢„æµ‹å™¨: è¾“å‡ºç»´åº¦={args.x_dim}")
    logging.info(f"â­ æœ€å¤§åºåˆ—é•¿åº¦: {max_num_exemplars} exemplars")

    # åˆ›å»ºTransformeré…ç½®
    config = transformer_lib_flax.TransformerConfig(
        num_heads=n_heads,
        num_layers=n_layers,
        hidden_size=hidden_size,
        dropout_rate=args.dropout_rate,
        attention_dropout_rate=args.attention_dropout_rate,
        loss_on_x_steps=args.loss_on_x_steps,
        norm_first=args.norm_first,
        disable_layer_norms=args.disable_layer_norms,
        final_layer_norm=args.final_layer_norm,
        kernel_init=transformer_lib_flax.nn_init_parser(args.kernel_init),
        bias_init=transformer_lib_flax.nn_init_parser(args.bias_init),
        linear_w_init=transformer_lib_flax.nn_init_parser(args.linear_w_init),
        linear_bias_init=transformer_lib_flax.nn_init_parser(args.linear_bias_init),
        posemb_init=transformer_lib_flax.nn_init_parser(args.posemb_init),
        max_len=(max_num_exemplars + 1) * 2,  # ä½¿ç”¨æœ€å¤§é•¿åº¦
        inner_dim=None,
        activation_fn=transformer_lib_flax.nn_activation_parser(args.activation_fn),
    )

    # â­ æ”¹åŠ¨3ï¼šä½¿ç”¨CausalLM_Wå¹¶ä¼ å…¥x_dimï¼ˆä½¿ç”¨w lossç‰ˆæœ¬ï¼‰
    model = predictor_flax_w_loss_w.CausalLM_W(config=config, x_dim=args.x_dim)

    @jax.jit
    def initialize_variables(init_rng):
        init_batch = jnp.ones((1, config.max_len, args.x_dim + 1), jnp.float32)
        init_variables = model.init(init_rng, inputs=init_batch, train=False)
        return init_variables

    init_variables = initialize_variables(init_rng)

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    if args.lr_scheduler_type == "cosine":
        scheduler = transformer_lib_flax.create_learning_rate_scheduler(
            base_learning_rate=args.learning_rate,
            num_warmup_steps=(args.n_epochs // 5) * args.n_iter_per_epoch,
            num_training_steps=args.n_epochs * args.n_iter_per_epoch,
        )
    elif args.lr_scheduler_type == "warmup":
        scheduler = transformer_lib_flax.create_learning_rate_scheduler_v2(
            factors="constant * linear_warmup",
            base_learning_rate=args.learning_rate,
            warmup_steps=(args.n_epochs // 5) * args.n_iter_per_epoch,
        )
    else:
        def scheduler(_):
            return args.learning_rate

    # åˆ›å»ºä¼˜åŒ–å™¨
    opt = optax.adamw(
        scheduler,
        b1=args.adam_b1,
        b2=args.adam_b2,
        eps=args.adam_eps,
        weight_decay=args.weight_decay,
    )

    # åˆ›å»ºè®­ç»ƒçŠ¶æ€
    state = train_state.TrainState.create(
        apply_fn=model.apply,
        params=init_variables["params"],
        tx=opt
    )

    # å¤åˆ¶åˆ°å¤šä¸ªè®¾å¤‡
    state = jax_utils.replicate(state)

    # åˆ›å»ºå¹¶è¡Œè®­ç»ƒæ­¥éª¤
    p_train_step = jax.pmap(
        lambda state, seq, task_ids, w_target, dropout_rng: train_step(state, seq, task_ids, w_target, model, scheduler, dropout_rng),
        axis_name="batch",
    )

    return model, state, p_train_step


def save_checkpoint(state, exp_folder):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    # unreplicateä¼šè‡ªåŠ¨å¤„ç†ï¼Œä¸éœ€è¦é¢å¤–çš„get_array
    state = jax_utils.unreplicate(state)
    ckpt_dir = os.path.abspath(os.path.join(exp_folder, "ckpt/"))
    gfile.makedirs(ckpt_dir)
    checkpoints.save_checkpoint(
        ckpt_dir,
        state,
        step=int(state.step),
        keep=3,
        overwrite=True,
    )
    logging.info(f"ä¿å­˜æ£€æŸ¥ç‚¹åˆ°: {ckpt_dir}")


def train_model(args):
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    utils.set_seed(args.seed)
    rng = random.PRNGKey(args.seed)
    rng, new_rng = random.split(rng)

    # åˆ›å»ºå®éªŒæ–‡ä»¶å¤¹
    gfile.makedirs(args.exp_folder)
    
    # ä¿å­˜é…ç½®
    with gfile.GFile(os.path.join(args.exp_folder, "config.json"), "w") as handle:
        json.dump(args.initial_dict, handle)

    # ç¡®å®šè®­ç»ƒæ¨¡å¼ï¼šå›ºå®šé•¿åº¦ vs å¯å˜é•¿åº¦
    if args.num_exemplars is not None:
        # å›ºå®šé•¿åº¦æ¨¡å¼
        use_variable_length = False
        train_num_exemplars = args.num_exemplars
        length_info = f"{args.num_exemplars} (å›ºå®š)"
    else:
        # å¯å˜é•¿åº¦æ¨¡å¼
        use_variable_length = True
        train_num_exemplars = args.max_num_exemplars  # åˆå§‹åŒ–samplerç”¨æœ€å¤§é•¿åº¦
        length_info = f"{args.min_num_exemplars}-{args.max_num_exemplars} (å‡åŒ€éšæœº)"

    logging.info("å¼€å§‹è®­ç»ƒ...")
    logging.info("="*70)
    logging.info("è®­ç»ƒé…ç½® (Wé¢„æµ‹ç‰ˆæœ¬ - ä½¿ç”¨W MSEæŸå¤±):")
    logging.info(f"  æ¨¡å‹: L=16, H=512, M=4")
    logging.info(f"  â­ é¢„æµ‹ç›®æ ‡: wå‘é‡ (ç»´åº¦={args.x_dim})")
    logging.info(f"  â­ æŸå¤±å‡½æ•°: MSE(w_pred, w_true) - ç›´æ¥ä¼˜åŒ–wçš„é¢„æµ‹")
    logging.info(f"  è®­ç»ƒ: {args.n_epochs * args.n_iter_per_epoch} iterations ({args.n_epochs} epochs Ã— {args.n_iter_per_epoch} iters)")
    logging.info(f"  æ•°æ®: {length_info} (x,y)å¯¹, x_dim={args.x_dim}, batch={args.batch_size}")
    logging.info(f"  åˆ†å¸ƒ: p(w)=N(0,I), p(x)=N(0,I)")
    logging.info(f"  ä¼˜åŒ–: lr={args.learning_rate}, scheduler={args.lr_scheduler_type}, Adam(Î²1={args.adam_b1}, Î²2={args.adam_b2})")
    logging.info(f"  Warmup: {args.n_epochs // 5} epochs (~{(args.n_epochs // 5) * args.n_iter_per_epoch} steps, 20% of training)")
    if use_variable_length:
        logging.info(f"  â­ å¯å˜é•¿åº¦è®­ç»ƒ: æ¯æ¬¡è¿­ä»£ä»[{args.min_num_exemplars}, {args.max_num_exemplars}]å‡åŒ€éšæœºæŠ½æ ·")
    logging.info("="*70)

    # åˆå§‹åŒ–æ¨¡å‹
    model, state, p_train_step = get_model(new_rng, args)

    # æ£€æŸ¥å¹¶æ¢å¤ checkpoint
    checkpoint_dir = os.path.abspath(os.path.join(args.exp_folder, "ckpt"))  # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    start_epoch = 0
    start_iteration = 0
    
    if gfile.exists(checkpoint_dir):
        try:
            # å…ˆ unreplicate ä»¥ä¾¿æ¢å¤
            state_unreplicated = jax_utils.unreplicate(state)
            # æ¢å¤æœ€æ–°çš„ checkpointï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
            restored_state = checkpoints.restore_checkpoint(checkpoint_dir, state_unreplicated)
            
            if restored_state is not None and hasattr(restored_state, 'step'):
                # è·å–å·²è®­ç»ƒçš„ step æ•°
                start_step = int(restored_state.step)
                start_epoch = start_step // args.n_iter_per_epoch
                start_iteration = start_step % args.n_iter_per_epoch
                
                # é‡æ–° replicate æ¢å¤çš„çŠ¶æ€
                state = jax_utils.replicate(restored_state)
                
                logging.info("="*70)
                logging.info("âœ… ä» checkpoint æ¢å¤è®­ç»ƒ")
                logging.info(f"  å·²å®Œæˆ: {start_step} steps = {start_epoch} epochs + {start_iteration} iterations")
                logging.info(f"  ç»§ç»­è®­ç»ƒ: ä» epoch {start_epoch}, iteration {start_iteration} å¼€å§‹")
                logging.info("="*70)
            else:
                logging.info("checkpoint æ— æ•ˆï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
        except Exception as e:
            logging.warning(f"æ¢å¤ checkpoint å¤±è´¥: {e}")
            logging.info("ä»å¤´å¼€å§‹è®­ç»ƒ")
    else:
        logging.info("æœªæ‰¾åˆ° checkpointï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")

    # åˆ›å»ºæ•°æ®é‡‡æ ·å™¨
    rng, new_rng = random.split(rng)
    # Parse task probabilities
    task_probs = [args.prob0, args.prob1, args.prob2, args.prob3]
    prob_sum = sum(task_probs)
    
    # Check if probabilities sum to 1.0
    if abs(prob_sum - 1.0) > 1e-6:
        raise ValueError(
            f"ä»»åŠ¡æ¦‚ç‡ä¹‹å’Œå¿…é¡»ç­‰äº1.0ï¼Œå½“å‰ä¸º {prob_sum}ã€‚\n"
            f"å½“å‰è®¾ç½®: prob0={args.prob0}, prob1={args.prob1}, prob2={args.prob2}, prob3={args.prob3}\n"
            f"è¯·è°ƒæ•´å‚æ•°ä½¿å…¶å’Œä¸º1.0"
        )
    
    logging.info(f"ğŸ“ ä»»åŠ¡æ¦‚ç‡è®¾ç½®: [Task1={args.prob0}, Task2={args.prob1}, Task3={args.prob2}, Task4={args.prob3}]")

    # å‡†å¤‡dropoutéšæœºæ•°
    dropout_rngs = random.split(new_rng, jax.local_device_count())
    
    # ç¡®å®šæœ€å¤§é•¿åº¦ç”¨äºpadding
    if args.num_exemplars is not None:
        max_len_for_padding = args.num_exemplars
    else:
        max_len_for_padding = args.max_num_exemplars

    # è®­ç»ƒå¾ªç¯
    metrics_history = []
    for epoch in range(start_epoch, args.n_epochs):
        epoch_metrics = []
        epoch_lengths = []  # è®°å½•æœ¬epochä½¿ç”¨çš„æ‰€æœ‰é•¿åº¦
        
        # å¦‚æœæ˜¯æ¢å¤è®­ç»ƒï¼Œç¬¬ä¸€ä¸ªepochä»start_iterationå¼€å§‹ï¼›å¦åˆ™ä»0å¼€å§‹
        start_iter = start_iteration if epoch == start_epoch else 0
        
        for iteration in range(start_iter, args.n_iter_per_epoch):
            # å¦‚æœæ˜¯å¯å˜é•¿åº¦æ¨¡å¼ï¼Œæ¯æ¬¡è¿­ä»£éšæœºé€‰æ‹©é•¿åº¦
            if use_variable_length:
                current_length = np.random.randint(args.min_num_exemplars, args.max_num_exemplars + 1)
                epoch_lengths.append(current_length)  # è®°å½•é•¿åº¦
            else:
                current_length = args.num_exemplars
            
            # åˆ›å»ºå½“å‰é•¿åº¦çš„sampler
            sampler = sampler_lib.Sampler(
                current_length,
                args.x_dim,
                args.hidden_size,
                x_distribution_fn=sampler_lib.str_to_distribution_fn(args.x_distribution_str),
                w_distribution_fn=sampler_lib.str_to_distribution_fn(args.w_distribution_str),
                noise_std=args.noise_std,
                task_probs=task_probs,
            )
            
            # é‡‡æ ·æ•°æ®
            seqs, coefficients, *_ = sampler.sample(n=args.batch_size)
            # è·å–ä»»åŠ¡ç±»å‹
            task_ids = sampler.get_last_task_ids()
            
            # å¦‚æœæ˜¯å¯å˜é•¿åº¦ï¼Œéœ€è¦paddingåˆ°æœ€å¤§é•¿åº¦
            if use_variable_length and current_length < max_len_for_padding:
                # seqs shape: (batch, current_length*2, x_dim+1)
                # éœ€è¦paddingåˆ° (batch, max_len_for_padding*2, x_dim+1)
                pad_length = (max_len_for_padding - current_length) * 2
                padding = np.zeros((seqs.shape[0], pad_length, seqs.shape[2]))
                seqs = np.concatenate([seqs, padding], axis=1)
            
            # è½¬æ¢ä¸ºJAXæ•°ç»„å¹¶åˆ†ç‰‡
            seqs = jnp.array(seqs)
            coefficients = jnp.array(coefficients)  # çœŸå®çš„wå‘é‡
            task_ids = jnp.array(task_ids, dtype=jnp.int32)
            seqs = common_utils.shard(seqs)
            coefficients = common_utils.shard(coefficients)  # â­ åˆ†ç‰‡wå‘é‡
            task_ids = common_utils.shard(task_ids)
            
            # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
            state, metrics = p_train_step(state, seqs, task_ids, coefficients, dropout_rng=dropout_rngs)
            
            # æ”¶é›†æŒ‡æ ‡
            metrics = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], metrics))
            # å¦‚æœæ˜¯å¯å˜é•¿åº¦ï¼Œè®°å½•è¯¥iterationçš„å®é™…é•¿åº¦ï¼Œç”¨äºåç»­æ­£ç¡®ç»Ÿè®¡
            if use_variable_length:
                metrics["actual_length"] = current_length
            epoch_metrics.append(metrics)
        
        # Epochç»“æŸï¼Œè®¡ç®—å¹³å‡æŒ‡æ ‡
        epoch_metrics = common_utils.stack_forest(epoch_metrics)
        avg_loss = jnp.mean(epoch_metrics["loss"])
        avg_lr = epoch_metrics["lr"][-1]
        
        # è·å–æœ€åä¸€ä¸ªiterationçš„æŒ‡æ ‡ï¼ˆç”¨äºæ˜¾ç¤ºå®é™…é•¿åº¦å’Œå¯¹åº”çš„lossï¼‰
        # epoch_metricsç»è¿‡stack_foreståï¼Œy_errorså’Œw_errorsçš„shapeæ˜¯(num_iterations, num_positions)
        last_y_errors = epoch_metrics["y_errors"][-1] / args.batch_size if epoch_metrics["y_errors"].shape[0] > 0 else jnp.array([])
        last_w_errors = epoch_metrics["w_errors"][-1] / args.batch_size if epoch_metrics["w_errors"].shape[0] > 0 else jnp.array([])
        
        # è·å–æœ€åä¸€ä¸ªiterationçš„å®é™…é•¿åº¦
        if use_variable_length and len(epoch_lengths) > 0:
            last_length = epoch_lengths[-1]
            # åªè¾“å‡ºåˆ°å®é™…é•¿åº¦ï¼Œä¸åŒ…å«paddingéƒ¨åˆ†
            last_y_errors = last_y_errors[:last_length]
            last_w_errors = last_w_errors[:last_length]
            
            # è¾“å‡ºé•¿åº¦ç»Ÿè®¡ï¼ˆæ•´ä¸ªepochçš„ç»Ÿè®¡ï¼‰
            avg_length = np.mean(epoch_lengths)
            min_length = np.min(epoch_lengths)
            max_length = np.max(epoch_lengths)
            std_length = np.std(epoch_lengths)
            # è®¡ç®—é•¿åº¦åˆ†å¸ƒï¼ˆæœ€å¤šæ˜¾ç¤ºå‰10ä¸ªæœ€å¸¸è§çš„é•¿åº¦ï¼‰
            length_counts = {}
            for length in epoch_lengths:
                length_counts[length] = length_counts.get(length, 0) + 1
            sorted_lengths = sorted(length_counts.items(), key=lambda x: x[1], reverse=True)[:10]
            length_str = ", ".join([f"{length}({count})" for length, count in sorted_lengths])
            logging.info(f"Epoch {epoch+1}/{args.n_epochs} - "
                        f"Loss (W MSE): {avg_loss:.6f}, "
                        f"LR: {avg_lr:.2e}")
            logging.info(f"  ğŸ“ åºåˆ—é•¿åº¦ç»Ÿè®¡: å¹³å‡={avg_length:.1f}Â±{std_length:.1f}, èŒƒå›´=[{min_length}, {max_length}], ä¸»è¦åˆ†å¸ƒ: {length_str}")
            logging.info(f"  ğŸ“ æœ€åæ‰¹æ¬¡: åºåˆ—é•¿åº¦={last_length}")
        else:
            last_length = args.num_exemplars if args.num_exemplars is not None else max_len_for_padding
        logging.info(f"Epoch {epoch+1}/{args.n_epochs} - "
                    f"Loss (W MSE): {avg_loss:.6f}, "
                    f"LR: {avg_lr:.2e}")
        
        # è¾“å‡ºæœ€åä¸€ä¸ªiterationçš„ä½ç½®lossæ•°ç»„ï¼ˆç®€æ´æ ¼å¼ï¼‰
        if len(last_w_errors) > 0:
            if len(last_w_errors) <= 100:
                w_loss_str = "[" + ", ".join([f"{float(last_w_errors[i]):.4f}" for i in range(len(last_w_errors))]) + "]"
                logging.info(f"Position W Loss (MSE, é•¿åº¦={last_length}): {w_loss_str}")
            else:
                logging.info(f"Position W Loss (MSE, é•¿åº¦={last_length}): (åºåˆ—å¤ªé•¿ï¼Œå…±{len(last_w_errors)}ä¸ªä½ç½®ï¼Œä»…æ˜¾ç¤ºå‰10ä¸ªå’Œå10ä¸ª)")
                w_first = ", ".join([f"{float(last_w_errors[i]):.4f}" for i in range(10)])
                w_last = ", ".join([f"{float(last_w_errors[i]):.4f}" for i in range(len(last_w_errors)-10, len(last_w_errors))])
                logging.info(f"  å‰10: [{w_first}]")
                logging.info(f"  å10: [{w_last}]")
        
        if len(last_y_errors) > 0:
            if len(last_y_errors) <= 100:
                y_loss_str = "[" + ", ".join([f"{float(last_y_errors[i]):.4f}" for i in range(len(last_y_errors))]) + "]"
                logging.info(f"Position Y Loss (MSE, é•¿åº¦={last_length}): {y_loss_str}")
            else:
                logging.info(f"Position Y Loss (MSE, é•¿åº¦={last_length}): (åºåˆ—å¤ªé•¿ï¼Œå…±{len(last_y_errors)}ä¸ªä½ç½®ï¼Œä»…æ˜¾ç¤ºå‰10ä¸ªå’Œå10ä¸ª)")
                y_first = ", ".join([f"{float(last_y_errors[i]):.4f}" for i in range(10)])
                y_last = ", ".join([f"{float(last_y_errors[i]):.4f}" for i in range(len(last_y_errors)-10, len(last_y_errors))])
                logging.info(f"  å‰10: [{y_first}]")
                logging.info(f"  å10: [{y_last}]")

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % 100 == 0:
            save_checkpoint(state, args.exp_folder)
        
        metrics_history.append(epoch_metrics)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    save_checkpoint(state, args.exp_folder)
    
    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
    metrics_history = common_utils.stack_forest(metrics_history)
    metrics_history["y_errors"] = jnp.mean(metrics_history["y_errors"], axis=0) / args.batch_size
    metrics_history["w_errors"] = jnp.mean(metrics_history["w_errors"], axis=0) / args.batch_size
    
    with gfile.GFile(os.path.join(args.exp_folder, "metrics.pickle"), "wb") as handle:
        pickle.dump(metrics_history, handle)
    
    logging.info("è®­ç»ƒå®Œæˆï¼")
    logging.info(f"æœ€ç»ˆW MSEæŸå¤±: {jnp.mean(metrics_history['loss'][-100:]):.6f}")
    logging.info(f"æœ€ç»ˆY MSEæŸå¤±: {jnp.mean(metrics_history['y_errors'][:, -1][-100:]):.6f}")
    
    return state, metrics_history


def main(_):
    """ä¸»å‡½æ•°"""
    args = utils.flags_to_args()
    train_model(args)


if __name__ == "__main__":
    app.run(main)

