#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•Wé¢„æµ‹å™¨ï¼šåˆ†æå„ä¸ªä½ç½®çš„wé¢„æµ‹å€¼å’Œloss
éªŒè¯ICLçš„å­¦ä¹ è¿‡ç¨‹ï¼šä»å…ˆéªŒåˆ†å¸ƒåˆ°çœŸå®w
"""

import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import json
import pickle
import numpy as np
from absl import app, flags, logging
from flax import jax_utils
from flax.training import checkpoints
from jax import random
import jax
import jax.numpy as jnp

from incontext import utils
from incontext import sampler_lib
from incontext import transformer_lib_flax
from incontext import predictor_flax_w

flags.DEFINE_string("checkpoint_dir", default="experiments/w_predictor/ckpt", help="æ£€æŸ¥ç‚¹ç›®å½•")
flags.DEFINE_integer("n_test_samples", default=500, help="æµ‹è¯•æ ·æœ¬æ•°")
flags.DEFINE_integer("seed", default=42, help="æµ‹è¯•éšæœºç§å­")
flags.DEFINE_string("output_file", default="experiments/w_predictor/w_analysis.pkl", help="è¾“å‡ºæ–‡ä»¶")
flags.DEFINE_string("test_x_distribution_str", default=None, help="æµ‹è¯•æ—¶xåˆ†å¸ƒï¼ˆNoneåˆ™ä½¿ç”¨è®­ç»ƒåˆ†å¸ƒï¼‰")
flags.DEFINE_string("test_w_distribution_str", default=None, help="æµ‹è¯•æ—¶wåˆ†å¸ƒï¼ˆNoneåˆ™ä½¿ç”¨è®­ç»ƒåˆ†å¸ƒï¼‰")
flags.DEFINE_float("test_prob0", default=None, help="æµ‹è¯•æ—¶ä»»åŠ¡1æ¦‚ç‡ï¼ˆNoneåˆ™ä½¿ç”¨è®­ç»ƒè®¾ç½®ï¼‰")
flags.DEFINE_float("test_prob1", default=None, help="æµ‹è¯•æ—¶ä»»åŠ¡2æ¦‚ç‡ï¼ˆNoneåˆ™ä½¿ç”¨è®­ç»ƒè®¾ç½®ï¼‰")
flags.DEFINE_float("test_prob2", default=None, help="æµ‹è¯•æ—¶ä»»åŠ¡3æ¦‚ç‡ï¼ˆNoneåˆ™ä½¿ç”¨è®­ç»ƒè®¾ç½®ï¼‰")
flags.DEFINE_float("test_prob3", default=None, help="æµ‹è¯•æ—¶ä»»åŠ¡4æ¦‚ç‡ï¼ˆNoneåˆ™ä½¿ç”¨è®­ç»ƒè®¾ç½®ï¼‰")
flags.DEFINE_integer("test_num_exemplars", default=None, help="æµ‹è¯•æ—¶åºåˆ—é•¿åº¦/ç¤ºä¾‹å¯¹æ•°é‡ï¼ˆNoneåˆ™ä½¿ç”¨è®­ç»ƒè®¾ç½®ï¼‰")
flags.DEFINE_float("test_task_mix_alpha", default=None, help="[å·²å¼ƒç”¨] æµ‹è¯•æ—¶ä»»åŠ¡æ··åˆæ¯”ä¾‹ï¼ˆNoneåˆ™ä½¿ç”¨è®­ç»ƒè®¾ç½®ï¼‰")
flags.DEFINE_bool("use_mc_dropout", default=False, help="æ˜¯å¦ä½¿ç”¨ MC Dropout è¿›è¡Œä¸ç¡®å®šæ€§ä¼°è®¡")
flags.DEFINE_integer("n_mc_samples", default=50, help="MC Dropout é‡‡æ ·æ¬¡æ•°ï¼ˆä»…åœ¨ use_mc_dropout=True æ—¶æœ‰æ•ˆï¼‰")

FLAGS = flags.FLAGS


def load_model_and_config(checkpoint_dir, test_max_len=None):
    """åŠ è½½æ¨¡å‹å’Œé…ç½®
    
    Args:
        checkpoint_dir: æ£€æŸ¥ç‚¹ç›®å½•
        test_max_len: æµ‹è¯•æ—¶çš„æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆå¦‚æœæŒ‡å®šï¼Œä¼šæ‰©å±•ä½ç½®ç¼–ç ä»¥æ”¯æŒæ›´é•¿åºåˆ—ï¼‰
    """
    # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    checkpoint_dir = os.path.abspath(checkpoint_dir)
    
    # åŠ è½½é…ç½®
    config_path = os.path.join(os.path.dirname(checkpoint_dir), "config.json")
    with open(config_path, 'r') as f:
        config_dict = json.load(f)
    
    args = utils.dict_to_args(config_dict)
    
    # åˆ›å»ºTransformeré…ç½®
    # å¤„ç†å¯å˜é•¿åº¦è®­ç»ƒçš„æƒ…å†µ
    if args.num_exemplars is not None:
        train_max_len = (args.num_exemplars + 1) * 2
    else:
        # å¯å˜é•¿åº¦æ¨¡å¼ï¼šä½¿ç”¨ max_num_exemplars
        train_max_len = (args.max_num_exemplars + 1) * 2
    
    # å¦‚æœæµ‹è¯•åºåˆ—é•¿åº¦è¶…è¿‡è®­ç»ƒæ—¶çš„max_lenï¼Œä½¿ç”¨æµ‹è¯•æ—¶çš„max_len
    actual_max_len = test_max_len if (test_max_len is not None and test_max_len > train_max_len) else train_max_len
    
    config = transformer_lib_flax.TransformerConfig(
        num_heads=getattr(args, 'n_heads', 4),
        num_layers=getattr(args, 'n_layers', 16),
        hidden_size=getattr(args, 'hidden_size', 512),
        loss_on_x_steps=args.loss_on_x_steps,
        norm_first=args.norm_first,
        disable_layer_norms=args.disable_layer_norms,
        final_layer_norm=args.final_layer_norm,
        kernel_init=transformer_lib_flax.nn_init_parser(args.kernel_init),
        bias_init=transformer_lib_flax.nn_init_parser(args.bias_init),
        linear_w_init=transformer_lib_flax.nn_init_parser(args.linear_w_init),
        linear_bias_init=transformer_lib_flax.nn_init_parser(args.linear_bias_init),
        posemb_init=transformer_lib_flax.nn_init_parser(args.posemb_init),
        max_len=actual_max_len,  # ä½¿ç”¨å®é™…éœ€è¦çš„æœ€å¤§é•¿åº¦
        inner_dim=None,
        activation_fn=transformer_lib_flax.nn_activation_parser(args.activation_fn),
        dropout_rate=getattr(args, 'dropout_rate', 0.0),  # â­ ä½¿ç”¨è®­ç»ƒæ—¶çš„dropout_rate
        attention_dropout_rate=getattr(args, 'attention_dropout_rate', 0.0),  # â­ ä½¿ç”¨è®­ç»ƒæ—¶çš„attention_dropout_rate
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = predictor_flax_w.CausalLM_W(config=config, x_dim=args.x_dim)
    
    # åˆå§‹åŒ–å˜é‡
    rng = random.PRNGKey(0)
    init_batch = jnp.ones((1, config.max_len, args.x_dim + 1), jnp.float32)
    init_variables = model.init(rng, inputs=init_batch, train=False)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    restored = checkpoints.restore_checkpoint(checkpoint_dir, target=None)
    params = restored['params']
    
    # å¦‚æœæµ‹è¯•åºåˆ—é•¿åº¦è¶…è¿‡è®­ç»ƒæ—¶çš„max_lenï¼Œéœ€è¦æ‰©å±•ä½ç½®ç¼–ç 
    if test_max_len is not None and test_max_len > train_max_len:
        # è·å–åŸå§‹ä½ç½®ç¼–ç ï¼ˆæ‰¾åˆ°æ­£ç¡®çš„é”®åï¼‰
        # å‚æ•°ç»“æ„å¯èƒ½æ˜¯ params['Transformer_0']['PositionEmbeddings_0']['pos_embedding']
        if 'Transformer_0' in params:
            original_pos_emb = params['Transformer_0']['PositionEmbeddings_0']['pos_embedding']
            transformer_key = 'Transformer_0'
        elif 'transformer' in params:
            original_pos_emb = params['transformer']['PositionEmbeddings_0']['pos_embedding']
            transformer_key = 'transformer'
        else:
            # å°è¯•æ‰¾åˆ°åŒ…å«PositionEmbeddingsçš„é”®
            transformer_key = None
            for key in params.keys():
                if isinstance(params[key], dict) and 'PositionEmbeddings_0' in params[key]:
                    original_pos_emb = params[key]['PositionEmbeddings_0']['pos_embedding']
                    transformer_key = key
                    break
            if transformer_key is None:
                raise KeyError(f"Cannot find position embeddings in params. Available keys: {list(params.keys())}")
        
        # æ£€æŸ¥ä½ç½®ç¼–ç åˆå§‹åŒ–æ–¹å¼
        posemb_init_fn = transformer_lib_flax.nn_init_parser(args.posemb_init)
        
        # ç”Ÿæˆæ‰©å±•çš„ä½ç½®ç¼–ç 
        hidden_size = config.hidden_size
        extended_shape = (1, test_max_len, hidden_size)
        
        # å¦‚æœæ˜¯æ­£å¼¦ä½ç½®ç¼–ç ï¼Œå¯ä»¥åŠ¨æ€ç”Ÿæˆ
        if 'sinusoidal' in str(args.posemb_init).lower() or 'sin' in str(args.posemb_init).lower():
            # ä½¿ç”¨æ­£å¼¦ä½ç½®ç¼–ç ç”Ÿæˆå™¨
            sinusoidal_init = transformer_lib_flax.sinusoidal_init(max_len=test_max_len)
            rng_pos = random.PRNGKey(0)
            extended_pos_emb = sinusoidal_init(rng_pos, extended_shape, jnp.float32)
        else:
            # å¯¹äºå¯å­¦ä¹ çš„ä½ç½®ç¼–ç ï¼Œæ‰©å±•åŸå§‹ç¼–ç å¹¶åˆå§‹åŒ–æ–°éƒ¨åˆ†
            # å…ˆå¤åˆ¶åŸå§‹ç¼–ç 
            extended_pos_emb = jnp.zeros(extended_shape, dtype=jnp.float32)
            extended_pos_emb = extended_pos_emb.at[:, :train_max_len, :].set(original_pos_emb)
            
            # å¯¹äºè¶…å‡ºéƒ¨åˆ†ï¼Œä½¿ç”¨åŸå§‹åˆå§‹åŒ–å™¨ç”Ÿæˆ
            new_pos_emb = posemb_init_fn(random.PRNGKey(1), (1, test_max_len - train_max_len, hidden_size), jnp.float32)
            extended_pos_emb = extended_pos_emb.at[:, train_max_len:, :].set(new_pos_emb)
        
        # æ›´æ–°å‚æ•°
        params[transformer_key]['PositionEmbeddings_0']['pos_embedding'] = extended_pos_emb
        
        logging.info(f"ğŸ”§ æ‰©å±•ä½ç½®ç¼–ç : {train_max_len} -> {test_max_len}")
    
    logging.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {checkpoint_dir}")
    logging.info(f"   é…ç½®: L={config.num_layers}, H={config.hidden_size}, M={config.num_heads}")
    logging.info(f"   æ•°æ®: {args.num_exemplars}ä¸ªæ ·æœ¬å¯¹, x_dim={args.x_dim}")
    if test_max_len is not None and test_max_len > train_max_len:
        logging.info(f"   æœ€å¤§åºåˆ—é•¿åº¦: {train_max_len} (è®­ç»ƒ) -> {test_max_len} (æµ‹è¯•)")
    
    # æ˜¾ç¤ºåˆ†å¸ƒé…ç½®
    x_dist = getattr(args, 'x_distribution_str', 'N/A')
    w_dist = getattr(args, 'w_distribution_str', 'N/A')
    logging.info(f"   åˆ†å¸ƒ: p(x)={x_dist}, p(w)={w_dist}")
    
    return model, params, args


def compute_covariance_matrices(samples):
    """
    è®¡ç®— MC Dropout æ ·æœ¬çš„åæ–¹å·®çŸ©é˜µ
    
    Args:
        samples: (n_mc, batch, num_exemplars, x_dim) - MC Dropout é‡‡æ ·ç»“æœ
    
    Returns:
        cov_matrices: (batch, num_exemplars, x_dim, x_dim) - åæ–¹å·®çŸ©é˜µ
    """
    n_mc, batch_size, num_exemplars, x_dim = samples.shape
    
    # è½¬ä¸º numpy ä»¥ä½¿ç”¨ np.cov
    samples_np = np.array(samples)
    
    # åˆå§‹åŒ–åæ–¹å·®çŸ©é˜µæ•°ç»„
    cov_matrices = np.zeros((batch_size, num_exemplars, x_dim, x_dim))
    
    # å¯¹æ¯ä¸ª (batch, position) è®¡ç®—åæ–¹å·®çŸ©é˜µ
    for i in range(batch_size):
        for j in range(num_exemplars):
            # æå–è¯¥æ ·æœ¬ã€è¯¥ä½ç½®çš„ MC é‡‡æ ·: (n_mc, x_dim)
            w_samples = samples_np[:, i, j, :]
            # è®¡ç®—åæ–¹å·®çŸ©é˜µ: (x_dim, x_dim)
            # np.cov æŒ‰è¡Œè®¡ç®—ï¼Œæ‰€ä»¥éœ€è¦è½¬ç½®
            cov_matrices[i, j, :, :] = np.cov(w_samples.T)
    
    return cov_matrices


def covariance_to_uncertainty(cov_matrices, method='trace'):
    """
    å°†åæ–¹å·®çŸ©é˜µæ˜ å°„ä¸ºæ ‡é‡ä¸ç¡®å®šæ€§åº¦é‡
    
    Args:
        cov_matrices: (batch, num_exemplars, x_dim, x_dim) æˆ– (num_exemplars, x_dim, x_dim)
                      åæ–¹å·®çŸ©é˜µ
        method: str - æ˜ å°„æ–¹æ³•
            - 'trace': è¿¹ï¼ˆæ€»æ–¹å·®ï¼‰
            - 'det': è¡Œåˆ—å¼çš„ n æ¬¡æ ¹ï¼ˆå‡ ä½•å¹³å‡ç‰¹å¾å€¼ï¼‰
            - 'max_eig': æœ€å¤§ç‰¹å¾å€¼
            - 'frobenius': Frobenius èŒƒæ•°
    
    Returns:
        uncertainty: (batch, num_exemplars,) æˆ– (num_exemplars,) - æ ‡é‡ä¸ç¡®å®šæ€§
    """
    # å¤„ç†è¾“å…¥å½¢çŠ¶
    if cov_matrices.ndim == 3:
        # (num_exemplars, x_dim, x_dim)
        batch_size = None
        num_exemplars, x_dim, _ = cov_matrices.shape
    else:
        # (batch, num_exemplars, x_dim, x_dim)
        batch_size, num_exemplars, x_dim, _ = cov_matrices.shape
    
    if method == 'trace':
        # è¿¹ï¼šæ€»æ–¹å·®
        if batch_size is None:
            uncertainty = np.array([np.trace(cov_matrices[j]) for j in range(num_exemplars)])
        else:
            uncertainty = np.array([[np.trace(cov_matrices[i, j]) 
                                   for j in range(num_exemplars)] 
                                  for i in range(batch_size)])
    
    elif method == 'det':
        # è¡Œåˆ—å¼çš„ n æ¬¡æ ¹
        if batch_size is None:
            uncertainty = np.array([np.linalg.det(cov_matrices[j]) ** (1/x_dim) 
                                  for j in range(num_exemplars)])
        else:
            uncertainty = np.array([[np.linalg.det(cov_matrices[i, j]) ** (1/x_dim)
                                   for j in range(num_exemplars)]
                                  for i in range(batch_size)])
    
    elif method == 'max_eig':
        # æœ€å¤§ç‰¹å¾å€¼
        if batch_size is None:
            uncertainty = np.array([np.max(np.linalg.eigvals(cov_matrices[j]).real)
                                  for j in range(num_exemplars)])
        else:
            uncertainty = np.array([[np.max(np.linalg.eigvals(cov_matrices[i, j]).real)
                                   for j in range(num_exemplars)]
                                  for i in range(batch_size)])
    
    elif method == 'frobenius':
        # Frobenius èŒƒæ•°
        if batch_size is None:
            uncertainty = np.array([np.linalg.norm(cov_matrices[j], 'fro')
                                  for j in range(num_exemplars)])
        else:
            uncertainty = np.array([[np.linalg.norm(cov_matrices[i, j], 'fro')
                                   for j in range(num_exemplars)]
                                  for i in range(batch_size)])
    
    else:
        raise ValueError(f"Unknown method: {method}. Supported: 'trace', 'det', 'max_eig', 'frobenius'")
    
    return uncertainty


def extract_w_predictions(model, params, seqs, num_exemplars, task_ids=None, use_mc_dropout=False, n_mc_samples=50, dropout_rng=None):
    """
    æå–å„ä¸ªä½ç½®çš„wé¢„æµ‹å€¼
    
    Args:
        model: æ¨¡å‹
        params: æ¨¡å‹å‚æ•°
        seqs: è¾“å…¥åºåˆ— (batch, seq_len, x_dim+1)
        num_exemplars: æ ·æœ¬å¯¹æ•°é‡
        task_ids: ä»»åŠ¡ç±»å‹ (batch,), å¯é€‰
        use_mc_dropout: æ˜¯å¦ä½¿ç”¨ MC Dropout
        n_mc_samples: MC Dropout é‡‡æ ·æ¬¡æ•°
        dropout_rng: dropout éšæœºæ•°ç”Ÿæˆå™¨
    
    Returns:
        w_preds: (batch, num_exemplars, x_dim) - å„ä½ç½®é¢„æµ‹çš„wï¼ˆMCæ¨¡å¼ä¸‹æ˜¯å‡å€¼ï¼‰
        y_preds: (batch, num_exemplars, 1) - å„ä½ç½®é¢„æµ‹çš„yï¼ˆMCæ¨¡å¼ä¸‹æ˜¯å‡å€¼ï¼‰
        y_errors: (batch, num_exemplars) - å„ä½ç½®çš„lossï¼ˆMCæ¨¡å¼ä¸‹æ˜¯å‡å€¼ï¼‰
        w_preds_cov: (batch, num_exemplars, x_dim, x_dim) - MCæ¨¡å¼ä¸‹wé¢„æµ‹çš„åæ–¹å·®çŸ©é˜µï¼Œå¦åˆ™None
        y_preds_std: (batch, num_exemplars, 1) - MCæ¨¡å¼ä¸‹yé¢„æµ‹çš„æ ‡å‡†å·®ï¼Œå¦åˆ™None
    """
    if use_mc_dropout:
        # MC Dropout: å¤šæ¬¡é‡‡æ ·
        w_preds_all = []
        y_preds_all = []
        y_errors_all = []
        
        for i in range(n_mc_samples):
            if (i + 1) % 10 == 0 or i == 0:
                logging.info(f"  é‡‡æ ·è¿›åº¦: {i+1}/{n_mc_samples}")
            dropout_rng, sub_rng = random.split(dropout_rng)
            errors, (y_errors, y_pred, seq_pred, seq_hiddens) = model.apply(
                {"params": params},
                inputs=seqs,
                task_ids=task_ids,
                train=True,  # â­ MC Dropout: æµ‹è¯•æ—¶ä¹Ÿå¯ç”¨ dropout
                rngs={"dropout": sub_rng},
                return_attention=False
            )
            
            # æå–wé¢„æµ‹
            w_preds = seq_pred[:, jnp.arange(0, seq_pred.shape[1], 2), :]
            w_preds_all.append(w_preds)
            y_preds_all.append(y_pred)
            y_errors_all.append(y_errors)
        
        # å †å å¹¶è®¡ç®—å‡å€¼
        w_preds_all = jnp.stack(w_preds_all, axis=0)  # (n_mc, batch, num_exemplars, x_dim)
        y_preds_all = jnp.stack(y_preds_all, axis=0)  # (n_mc, batch, num_exemplars, 1)
        y_errors_all = jnp.stack(y_errors_all, axis=0)  # (n_mc, batch, num_exemplars)
        
        w_preds = jnp.mean(w_preds_all, axis=0)  # (batch, num_exemplars, x_dim)
        y_preds = jnp.mean(y_preds_all, axis=0)  # (batch, num_exemplars, 1)
        y_errors = jnp.mean(y_errors_all, axis=0)  # (batch, num_exemplars)
        
        # è®¡ç®—åæ–¹å·®çŸ©é˜µå’Œyçš„æ ‡å‡†å·®
        logging.info("  è®¡ç®—åæ–¹å·®çŸ©é˜µ...")
        w_preds_cov = compute_covariance_matrices(w_preds_all)  # (batch, num_exemplars, x_dim, x_dim)
        y_preds_std = jnp.std(y_preds_all, axis=0)  # (batch, num_exemplars, 1)
        
        return w_preds, y_preds, y_errors, w_preds_cov, y_preds_std
    else:
        # æ ‡å‡†æ¨¡å¼
        errors, (y_errors, y_pred, seq_pred, seq_hiddens) = model.apply(
            {"params": params},
            inputs=seqs,
            task_ids=task_ids,
            train=False,
            return_attention=False
        )
        
        # seq_pred shape: (batch, seq_len-1, x_dim)
        # æå–yä½ç½®çš„wé¢„æµ‹ (å¶æ•°ä½ç½®ï¼š0, 2, 4, ...)
        w_preds = seq_pred[:, jnp.arange(0, seq_pred.shape[1], 2), :]
        # w_preds shape: (batch, num_exemplars, x_dim)
        
        return w_preds, y_pred, y_errors, None, None


def analyze_w_predictions(w_preds, w_true, y_errors, x_dim):
    """
    åˆ†æwé¢„æµ‹å€¼çš„ç»Ÿè®¡ç‰¹æ€§
    
    Args:
        w_preds: (n_samples, num_exemplars, x_dim) - é¢„æµ‹çš„w
        w_true: (n_samples, x_dim) - çœŸå®çš„w
        y_errors: (n_samples, num_exemplars) - å„ä½ç½®loss
        x_dim: xçš„ç»´åº¦
    
    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    n_samples, num_exemplars, _ = w_preds.shape
    
    # è®¡ç®—å„ä½ç½®wé¢„æµ‹çš„å‡å€¼å‘é‡å’Œåæ–¹å·®çŸ©é˜µ
    w_mean_per_pos = np.mean(w_preds, axis=0)  # (num_exemplars, x_dim)
    w_cov_per_pos = []  # åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (x_dim, x_dim) çš„åæ–¹å·®çŸ©é˜µ
    for pos in range(num_exemplars):
        # w_preds[:, pos, :] æ˜¯ (n_samples, x_dim)
        cov_matrix = np.cov(w_preds[:, pos, :], rowvar=False)  # (x_dim, x_dim)
        w_cov_per_pos.append(cov_matrix)
    w_cov_per_pos = np.array(w_cov_per_pos)  # (num_exemplars, x_dim, x_dim)
    
    # è®¡ç®—å„ä½ç½®wé¢„æµ‹ä¸çœŸå®wçš„MSE
    w_true_expanded = w_true[:, None, :]  # (n_samples, 1, x_dim)
    w_mse_per_pos = np.mean((w_preds - w_true_expanded)**2, axis=(0, 2))  # (num_exemplars,)
    
    # è®¡ç®—å„ä½ç½®wé¢„æµ‹ä¸çœŸå®wçš„ä½™å¼¦ç›¸ä¼¼åº¦
    w_norm = np.linalg.norm(w_preds, axis=2, keepdims=True)  # (n_samples, num_exemplars, 1)
    w_true_norm = np.linalg.norm(w_true, axis=1, keepdims=True)  # (n_samples, 1)
    w_normalized = w_preds / (w_norm + 1e-8)
    w_true_normalized = w_true / (w_true_norm + 1e-8)
    
    cosine_sim = np.sum(w_normalized * w_true_normalized[:, None, :], axis=2)  # (n_samples, num_exemplars)
    cosine_sim_mean = np.mean(cosine_sim, axis=0)  # (num_exemplars,)
    
    # å¹³å‡y loss
    avg_y_loss = np.mean(y_errors, axis=0)  # (num_exemplars,)
    
    return {
        'w_mean_per_pos': w_mean_per_pos,             # å„ä½ç½®wçš„å‡å€¼å‘é‡ (num_exemplars, x_dim)
        'w_cov_per_pos': w_cov_per_pos,               # å„ä½ç½®wçš„åæ–¹å·®çŸ©é˜µ (num_exemplars, x_dim, x_dim)
        'w_mse_per_pos': w_mse_per_pos,               # å„ä½ç½®wé¢„æµ‹çš„MSE
        'cosine_sim_mean': cosine_sim_mean,           # å„ä½ç½®wä¸çœŸå®wçš„ä½™å¼¦ç›¸ä¼¼åº¦
        'avg_y_loss': avg_y_loss,                     # å„ä½ç½®yçš„å¹³å‡loss
        'w_preds': w_preds,                           # æ‰€æœ‰wé¢„æµ‹å€¼ï¼ˆç”¨äºè¿›ä¸€æ­¥åˆ†æï¼‰
        'w_true': w_true,                             # çœŸå®wå€¼
    }


def test_w_predictor(args):
    """æµ‹è¯•Wé¢„æµ‹å™¨"""
    # è®¾ç½®éšæœºç§å­
    utils.set_seed(args.seed)
    rng = random.PRNGKey(args.seed)
    
    # å…ˆåŠ è½½é…ç½®ä»¥è·å–è®­ç»ƒæ—¶çš„åºåˆ—é•¿åº¦
    checkpoint_dir = os.path.abspath(args.checkpoint_dir)
    config_path = os.path.join(os.path.dirname(checkpoint_dir), "config.json")
    with open(config_path, 'r') as f:
        config_dict = json.load(f)
    train_args = utils.dict_to_args(config_dict)
    
    # ç¡®å®šæµ‹è¯•åºåˆ—é•¿åº¦ï¼ˆä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨è®­ç»ƒè®¾ç½®ï¼‰
    test_num_exemplars = args.test_num_exemplars if args.test_num_exemplars is not None else train_args.num_exemplars
    test_max_len = (test_num_exemplars + 1) * 2
    
    if test_num_exemplars != train_args.num_exemplars:
        logging.info(f"ğŸ“ ä½¿ç”¨æµ‹è¯•åºåˆ—é•¿åº¦: {test_num_exemplars} (è®­ç»ƒæ—¶: {train_args.num_exemplars})")
    else:
        logging.info(f"ğŸ“ ä½¿ç”¨è®­ç»ƒåºåˆ—é•¿åº¦: {test_num_exemplars}")
    
    # åŠ è½½æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦æ‰©å±•ä½ç½®ç¼–ç ï¼Œä¼šåœ¨è¿™é‡Œå¤„ç†ï¼‰
    model, params, train_args = load_model_and_config(args.checkpoint_dir, test_max_len=test_max_len)
    
    # ç¡®å®šæµ‹è¯•ä½¿ç”¨çš„åˆ†å¸ƒï¼ˆä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨è®­ç»ƒåˆ†å¸ƒï¼‰
    test_x_dist_str = args.test_x_distribution_str if args.test_x_distribution_str else train_args.x_distribution_str
    test_w_dist_str = args.test_w_distribution_str if args.test_w_distribution_str else train_args.w_distribution_str
    
    # ç¡®å®šä»»åŠ¡æ¦‚ç‡ï¼ˆä¼˜å…ˆä½¿ç”¨æµ‹è¯•æŒ‡å®šçš„å€¼ï¼Œå¦åˆ™ä½¿ç”¨è®­ç»ƒè®¾ç½®ï¼‰
    if any(p is not None for p in [args.test_prob0, args.test_prob1, args.test_prob2, args.test_prob3]):
        # ä½¿ç”¨æµ‹è¯•æ¦‚ç‡ï¼ˆæœªæŒ‡å®šçš„é»˜è®¤ä¸ºè®­ç»ƒå€¼ï¼‰
        test_prob0 = args.test_prob0 if args.test_prob0 is not None else getattr(train_args, 'prob0', 1.0)
        test_prob1 = args.test_prob1 if args.test_prob1 is not None else getattr(train_args, 'prob1', 0.0)
        test_prob2 = args.test_prob2 if args.test_prob2 is not None else getattr(train_args, 'prob2', 0.0)
        test_prob3 = args.test_prob3 if args.test_prob3 is not None else getattr(train_args, 'prob3', 0.0)
        
        task_probs = [test_prob0, test_prob1, test_prob2, test_prob3]
        prob_sum = sum(task_probs)
        
        if abs(prob_sum - 1.0) > 1e-6:
            raise ValueError(
                f"æµ‹è¯•ä»»åŠ¡æ¦‚ç‡ä¹‹å’Œå¿…é¡»ç­‰äº1.0ï¼Œå½“å‰ä¸º {prob_sum}ã€‚\n"
                f"å½“å‰è®¾ç½®: test_prob0={test_prob0}, test_prob1={test_prob1}, "
                f"test_prob2={test_prob2}, test_prob3={test_prob3}\n"
                f"è¯·è°ƒæ•´å‚æ•°ä½¿å…¶å’Œä¸º1.0"
            )
        
        logging.info(f"ğŸ“ ä½¿ç”¨æµ‹è¯•ä»»åŠ¡æ¦‚ç‡: [Task1={test_prob0}, Task2={test_prob1}, Task3={test_prob2}, Task4={test_prob3}]")
    else:
        # ä½¿ç”¨è®­ç»ƒæ¦‚ç‡
        train_prob0 = getattr(train_args, 'prob0', 1.0)
        train_prob1 = getattr(train_args, 'prob1', 0.0)
        train_prob2 = getattr(train_args, 'prob2', 0.0)
        train_prob3 = getattr(train_args, 'prob3', 0.0)
        task_probs = [train_prob0, train_prob1, train_prob2, train_prob3]
        logging.info(f"ğŸ“ ä½¿ç”¨è®­ç»ƒæ—¶çš„ä»»åŠ¡æ¦‚ç‡: [Task1={train_prob0}, Task2={train_prob1}, Task3={train_prob2}, Task4={train_prob3}]")
    
    # åˆ›å»ºæ•°æ®é‡‡æ ·å™¨
    sampler = sampler_lib.Sampler(
        test_num_exemplars,  # ä½¿ç”¨æµ‹è¯•åºåˆ—é•¿åº¦
        train_args.x_dim,
        train_args.hidden_size,
        x_distribution_fn=sampler_lib.str_to_distribution_fn(test_x_dist_str),
        w_distribution_fn=sampler_lib.str_to_distribution_fn(test_w_dist_str),
        noise_std=train_args.noise_std,
        task_probs=task_probs,
    )
    
    logging.info(f"ğŸ§ª å¼€å§‹æµ‹è¯•ï¼Œç”Ÿæˆ {args.n_test_samples} ä¸ªæµ‹è¯•æ ·æœ¬...")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    seqs, coefficients, xs, ys = sampler.sample(n=args.n_test_samples)
    # è·å–ä»»åŠ¡ç±»å‹
    task_ids = sampler.get_last_task_ids()
    
    seqs = jnp.array(seqs)
    coefficients = jnp.array(coefficients)  # çœŸå®çš„w
    xs_true = np.array(xs)  # (n_samples, test_num_exemplars, x_dim) - ä¿å­˜çœŸå®xå€¼
    ys_true = np.array(ys)  # (n_samples, test_num_exemplars, 1) - ä¿å­˜çœŸå®yå€¼
    task_ids_np = np.array(task_ids) if task_ids is not None else None  # ä¿å­˜task_ids
    task_ids = jnp.array(task_ids, dtype=jnp.int32) if task_ids is not None else None
    
    # æå–wé¢„æµ‹
    if args.use_mc_dropout:
        # æ£€æŸ¥è®­ç»ƒæ—¶æ˜¯å¦å¯ç”¨äº† dropout
        train_dropout = getattr(train_args, 'dropout_rate', 0.0)
        train_attention_dropout = getattr(train_args, 'attention_dropout_rate', 0.0)
        if train_dropout == 0.0:
            logging.warning("âš ï¸  è­¦å‘Š: è®­ç»ƒæ—¶ dropout_rate=0.0ï¼ŒMC Dropout å¯èƒ½æ— æ•ˆï¼")
            logging.warning("     å»ºè®®é‡æ–°è®­ç»ƒæ¨¡å‹å¹¶è®¾ç½® dropout_rate > 0 (æ¨è 0.1)")
        else:
            logging.info(f"ğŸ“Š MC Dropout é…ç½®:")
            logging.info(f"   è®­ç»ƒæ—¶ dropout_rate: {train_dropout}")
            logging.info(f"   è®­ç»ƒæ—¶ attention_dropout_rate: {train_attention_dropout}")
            logging.info(f"   æµ‹è¯•æ—¶ä½¿ç”¨ç›¸åŒçš„ dropout rate (MC Dropout è¦æ±‚)")
        
        logging.info(f"ğŸ“Š ä½¿ç”¨ MC Dropout æå–å„ä½ç½®çš„wé¢„æµ‹å€¼ (é‡‡æ · {args.n_mc_samples} æ¬¡)...")
    else:
        logging.info("ğŸ“Š æå–å„ä½ç½®çš„wé¢„æµ‹å€¼...")
    
    rng, dropout_rng = random.split(rng)
    w_preds, y_preds, y_errors, w_preds_cov, y_preds_std = extract_w_predictions(
        model, params, seqs, test_num_exemplars, 
        task_ids=task_ids,
        use_mc_dropout=args.use_mc_dropout,
        n_mc_samples=args.n_mc_samples if args.use_mc_dropout else 50,
        dropout_rng=dropout_rng if args.use_mc_dropout else None
    )
    
    if args.use_mc_dropout:
        logging.info(f"âœ“ MC Dropout é‡‡æ ·å®Œæˆ")
    
    # è½¬ä¸ºnumpy
    w_preds = np.array(w_preds)
    y_preds = np.array(y_preds)  # (n_samples, test_num_exemplars, 1) - Wé¢„æµ‹å™¨é¢„æµ‹çš„y
    y_errors = np.array(y_errors)
    coefficients = np.array(coefficients)
    
    # MC Dropout ä¸ç¡®å®šæ€§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if args.use_mc_dropout and w_preds_cov is not None:
        # w_preds_cov å·²ç»æ˜¯ numpy æ•°ç»„: (n_samples, test_num_exemplars, x_dim, x_dim)
        y_preds_std = np.array(y_preds_std)  # (n_samples, test_num_exemplars, 1)
    else:
        w_preds_cov = None
        y_preds_std = None
    
    # è®¡ç®—Wé¢„æµ‹å™¨é¢„æµ‹çš„yçš„å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆç”¨äºä¸Yé¢„æµ‹å™¨å¯¹æ¯”ï¼‰
    y_pred_mean_per_pos = np.mean(y_preds, axis=0)  # (test_num_exemplars, 1)
    y_pred_std_per_pos = np.std(y_preds, axis=0)    # (test_num_exemplars, 1)
    
    # è®¡ç®—yçœŸå®å€¼çš„å‡å€¼å’Œæ ‡å‡†å·®
    y_true_mean_per_pos = np.mean(ys_true, axis=0)  # (test_num_exemplars, 1)
    y_true_std_per_pos = np.std(ys_true, axis=0)    # (test_num_exemplars, 1)
    
    # åˆ†æç»“æœ
    logging.info("ğŸ” åˆ†æwé¢„æµ‹å€¼çš„ç»Ÿè®¡ç‰¹æ€§...")
    analysis = analyze_w_predictions(w_preds, coefficients, y_errors, train_args.x_dim)
    
    # æ·»åŠ yé¢„æµ‹å€¼å’ŒçœŸå®å€¼ç»Ÿè®¡ï¼ˆç”¨äºä¸Yé¢„æµ‹å™¨å¯¹æ¯”ï¼‰
    analysis['y_pred_mean_per_pos'] = y_pred_mean_per_pos
    analysis['y_pred_std_per_pos'] = y_pred_std_per_pos
    analysis['y_true_mean_per_pos'] = y_true_mean_per_pos
    analysis['y_true_std_per_pos'] = y_true_std_per_pos
    
    # è¾“å‡ºç»“æœ
    logging.info("\n" + "="*70)
    logging.info("æµ‹è¯•ç»“æœåˆ†æ")
    if args.use_mc_dropout:
        logging.info(f"æ¨¡å¼: MC Dropout (é‡‡æ · {args.n_mc_samples} æ¬¡)")
    else:
        logging.info("æ¨¡å¼: æ ‡å‡†æµ‹è¯•")
    logging.info("="*70)
    
    # è¾“å‡ºå„ä½ç½®çš„lossï¼ˆå’Œè®­ç»ƒæ—¶æ ¼å¼ä¸€è‡´ï¼‰
    avg_y_loss = analysis['avg_y_loss']
    loss_str = "[" + ", ".join([f"{float(avg_y_loss[i]):.4f}" for i in range(len(avg_y_loss))]) + "]"
    logging.info(f"\nå„ä½ç½®Yé¢„æµ‹Loss: {loss_str}")
    
    # è¾“å‡ºå„ä½ç½®wé¢„æµ‹çš„MSE
    w_mse = analysis['w_mse_per_pos']
    mse_str = "[" + ", ".join([f"{float(w_mse[i]):.4f}" for i in range(len(w_mse))]) + "]"
    logging.info(f"\nå„ä½ç½®Wé¢„æµ‹MSE:  {mse_str}")
    
    # è¾“å‡ºå„ä½ç½®wä¸çœŸå®wçš„ä½™å¼¦ç›¸ä¼¼åº¦
    cosine_sim = analysis['cosine_sim_mean']
    cosine_str = "[" + ", ".join([f"{float(cosine_sim[i]):.4f}" for i in range(len(cosine_sim))]) + "]"
    logging.info(f"\nå„ä½ç½®Wä½™å¼¦ç›¸ä¼¼åº¦: {cosine_str}")
    
    # MC Dropout ä¸ç¡®å®šæ€§ç»Ÿè®¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if args.use_mc_dropout and w_preds_cov is not None:
        # Wä¸ç¡®å®šæ€§ï¼šå¯¹æ‰€æœ‰æµ‹è¯•æ ·æœ¬æ±‚å¹³å‡å¾—åˆ°å¹³å‡åæ–¹å·®çŸ©é˜µï¼Œå†è½¬æ¢ä¸ºæ ‡é‡ä¸ç¡®å®šæ€§
        w_cov_mean = np.mean(w_preds_cov, axis=0)  # (test_num_exemplars, x_dim, x_dim)
        # ä½¿ç”¨ trace æ–¹æ³•ï¼ˆæ€»æ–¹å·®ï¼‰å°†åæ–¹å·®çŸ©é˜µè½¬æ¢ä¸ºæ ‡é‡ä¸ç¡®å®šæ€§
        w_uncertainty_per_pos = covariance_to_uncertainty(w_cov_mean, method='trace')  # (test_num_exemplars,)
        y_uncertainty_per_pos = np.mean(y_preds_std, axis=0).squeeze()  # (test_num_exemplars,)
        
        logging.info("\n" + "-"*70)
        logging.info("MC Dropout ä¸ç¡®å®šæ€§åˆ†æ (åŸºäºåæ–¹å·®çŸ©é˜µ)")
        logging.info("-"*70)
        logging.info(f"W å¹³å‡ä¸ç¡®å®šæ€§: {np.mean(w_uncertainty_per_pos):.6f}")
        logging.info(f"W ç¬¬1ä¸ªä½ç½®ä¸ç¡®å®šæ€§: {w_uncertainty_per_pos[0]:.6f}")
        logging.info(f"W æœ€åä½ç½®ä¸ç¡®å®šæ€§: {w_uncertainty_per_pos[-1]:.6f}")
        logging.info(f"W ä¸ç¡®å®šæ€§ä¸‹é™: {w_uncertainty_per_pos[0] - w_uncertainty_per_pos[-1]:.6f} ({(1 - w_uncertainty_per_pos[-1]/w_uncertainty_per_pos[0])*100:.1f}%)")
        logging.info(f"Y å¹³å‡ä¸ç¡®å®šæ€§: {np.mean(y_uncertainty_per_pos):.6f}")
        logging.info(f"Y ç¬¬1ä¸ªä½ç½®ä¸ç¡®å®šæ€§: {y_uncertainty_per_pos[0]:.6f}")
        logging.info(f"Y æœ€åä½ç½®ä¸ç¡®å®šæ€§: {y_uncertainty_per_pos[-1]:.6f}")
        logging.info(f"Y ä¸ç¡®å®šæ€§ä¸‹é™: {y_uncertainty_per_pos[0] - y_uncertainty_per_pos[-1]:.6f} ({(1 - y_uncertainty_per_pos[-1]/y_uncertainty_per_pos[0])*100:.1f}%)")
        w_uncertainty_str = "[" + ", ".join([f"{float(w_uncertainty_per_pos[i]):.4f}" for i in range(len(w_uncertainty_per_pos))]) + "]"
        y_uncertainty_str = "[" + ", ".join([f"{float(y_uncertainty_per_pos[i]):.4f}" for i in range(len(y_uncertainty_per_pos))]) + "]"
        logging.info(f"å„ä½ç½®Wä¸ç¡®å®šæ€§: {w_uncertainty_str}")
        logging.info(f"å„ä½ç½®Yä¸ç¡®å®šæ€§: {y_uncertainty_str}")
    
    # è¾“å‡ºæµ‹è¯•åˆ†å¸ƒä¿¡æ¯
    logging.info("\n" + "="*70)
    train_x_dist = getattr(train_args, 'x_distribution_str', 'N/A')
    train_w_dist = getattr(train_args, 'w_distribution_str', 'N/A')
    logging.info(f"è®­ç»ƒåˆ†å¸ƒ: p(x)={train_x_dist}, p(w)={train_w_dist}")
    logging.info(f"æµ‹è¯•åˆ†å¸ƒ: p(x)={test_x_dist_str}, p(w)={test_w_dist_str}")
    if test_x_dist_str != train_x_dist or test_w_dist_str != train_w_dist:
        logging.info("âš ï¸  æ³¨æ„: æµ‹è¯•åˆ†å¸ƒä¸è®­ç»ƒåˆ†å¸ƒä¸åŒ (Out-of-Distribution æµ‹è¯•)")
    logging.info("="*70)
    
    # ä¿å­˜ç»“æœï¼ˆåŒ…å«åˆ†å¸ƒä¿¡æ¯å’ŒåŸå§‹æ•°æ®ï¼‰
    os.makedirs(os.path.dirname(args.output_file), exist_ok=True)
    analysis['train_x_distribution_str'] = train_x_dist
    analysis['train_w_distribution_str'] = train_w_dist
    analysis['test_x_distribution_str'] = test_x_dist_str
    analysis['test_w_distribution_str'] = test_w_dist_str
    analysis['exp_folder'] = getattr(train_args, 'exp_folder', 'N/A')
    
    # MC Dropout ä¿¡æ¯
    if args.use_mc_dropout:
        analysis['use_mc_dropout'] = True
        analysis['n_mc_samples'] = args.n_mc_samples
        if w_preds_cov is not None:
            analysis['w_preds_cov_mc'] = w_preds_cov  # MC Dropout Wåæ–¹å·®çŸ©é˜µ
            analysis['y_preds_std_mc'] = y_preds_std  # MC Dropout Yä¸ç¡®å®šæ€§
            # Wä¸ç¡®å®šæ€§ï¼šå¯¹æ‰€æœ‰æµ‹è¯•æ ·æœ¬æ±‚å¹³å‡å¾—åˆ°å¹³å‡åæ–¹å·®çŸ©é˜µï¼Œå†è½¬æ¢ä¸ºæ ‡é‡ä¸ç¡®å®šæ€§
            w_cov_mean = np.mean(w_preds_cov, axis=0)  # (test_num_exemplars, x_dim, x_dim)
            w_uncertainty_per_pos = covariance_to_uncertainty(w_cov_mean, method='trace')  # (test_num_exemplars,)
            y_uncertainty_per_pos = np.mean(y_preds_std, axis=0).squeeze()
            analysis['w_cov_mean'] = w_cov_mean  # ä¿å­˜å¹³å‡åæ–¹å·®çŸ©é˜µ
            analysis['w_uncertainty_per_pos'] = w_uncertainty_per_pos
            analysis['y_uncertainty_per_pos'] = y_uncertainty_per_pos
    else:
        analysis['use_mc_dropout'] = False
    
    # ä¿å­˜åŸå§‹æ•°æ®ç”¨äºè¯¦ç»†åˆ†æ
    analysis['xs_true'] = xs_true  # (n_samples, test_num_exemplars, x_dim)
    analysis['ys_true'] = ys_true  # (n_samples, test_num_exemplars, 1)
    analysis['task_ids'] = task_ids_np  # (n_samples,)
    with open(args.output_file, 'wb') as f:
        pickle.dump(analysis, f)
    
    logging.info(f"\nâœ… åˆ†æç»“æœå·²ä¿å­˜åˆ°: {args.output_file}")
    logging.info("="*70)
    
    return analysis


def main(_):
    """ä¸»å‡½æ•°"""
    args = utils.flags_to_args()
    test_w_predictor(args)


if __name__ == "__main__":
    app.run(main)
