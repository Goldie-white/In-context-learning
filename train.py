#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸Šä¸‹æ–‡å­¦ä¹ è®­ç»ƒè„šæœ¬ - åŸºäºå®Œæ•´é¡¹ç›®ç†è§£é‡å†™
ä¸“æ³¨äºTransformeræ¨¡å‹çš„è®­ç»ƒï¼Œä¸åŒ…å«ç”»å›¾å’Œå¤æ‚è¯„ä¼°åŠŸèƒ½
"""

import os
# å¼ºåˆ¶ä½¿ç”¨å•GPUé¿å…NCCLå¤šGPUé—®é¢˜
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
import json
import pickle
from absl import app, flags, logging
from flax import jax_utils
from flax.training import train_state
from flax.training import common_utils
from flax.training import checkpoints
from jax import random
import jax
import jax.numpy as jnp
import optax
from tensorflow.io import gfile

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from incontext import utils
from incontext import sampler_lib
from incontext import transformer_lib_flax
from incontext import predictor_flax

# åŸºç¡€è®­ç»ƒå‚æ•° (ä¸åŸå§‹é¡¹ç›®model_trainer.pyå’Œmain.pyä¸€è‡´)
flags.DEFINE_integer("seed", default=0, help="éšæœºç§å­")
flags.DEFINE_integer("batch_size", default=64, help="æ‰¹æ¬¡å¤§å°")
flags.DEFINE_integer("x_dim", default=20, help="è¾“å…¥ç»´åº¦")
flags.DEFINE_integer("num_exemplars", default=40, help="ç¤ºä¾‹æ•°é‡")
flags.DEFINE_integer("n_epochs", default=5000, help="è®­ç»ƒè½®æ•°")  # åŸå§‹é¡¹ç›®: 5000 epochs
flags.DEFINE_integer("n_iter_per_epoch", default=100, help="æ¯è½®è¿­ä»£æ¬¡æ•°")  # åŸå§‹é¡¹ç›®: 100 iters, æ€»å…±~500Kæ­¥
flags.DEFINE_float("learning_rate", default=1e-4, help="å­¦ä¹ ç‡")  # åŸå§‹é¡¹ç›®: 1e-4
flags.DEFINE_float("weight_decay", default=0, help="æƒé‡è¡°å‡")  # åŸå§‹é¡¹ç›®: 0
flags.DEFINE_string("exp_folder", default="experiments/y_predictor", help="å®éªŒæ–‡ä»¶å¤¹")

# æ•°æ®åˆ†å¸ƒå‚æ•°
flags.DEFINE_string("x_distribution_str", default="normal*1.0+0.0", help="è¾“å…¥åˆ†å¸ƒ")
flags.DEFINE_string("w_distribution_str", default="normal*1.0+0.0", help="æƒé‡åˆ†å¸ƒ")
flags.DEFINE_float("noise_std", default=0.0, help="å™ªå£°æ ‡å‡†å·®")
flags.DEFINE_float("prob0", default=1.0, help="ä»»åŠ¡1æ¦‚ç‡: y=w^Tx (æ ‡å‡†çº¿æ€§å›å½’)")
flags.DEFINE_float("prob1", default=0.0, help="ä»»åŠ¡2æ¦‚ç‡: y=w^Tsort(x) (æ’åºçº¿æ€§å›å½’)")
flags.DEFINE_float("prob2", default=0.0, help="ä»»åŠ¡3æ¦‚ç‡: y=(dim/sqrt(2))*w^Tsoftmax(x) (ç¼©æ”¾softmaxçº¿æ€§å›å½’)")
flags.DEFINE_float("prob3", default=0.0, help="ä»»åŠ¡4æ¦‚ç‡: y=||x-w||^2 (å¹³æ–¹è·ç¦»)")
flags.DEFINE_float("task_mix_alpha", default=1.0, help="[å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨prob0-prob3] ä»»åŠ¡æ··åˆæ¯”ä¾‹")
flags.DEFINE_float("task3_prob", default=0.0, help="[å·²å¼ƒç”¨ï¼Œè¯·ä½¿ç”¨prob0-prob3] ä»»åŠ¡3æ¦‚ç‡")

# Transformeræ¨¡å‹å‚æ•° (åœ¨ transformer_lib_flax.py ä¸­å®šä¹‰)

# ä¼˜åŒ–å™¨å‚æ•°
flags.DEFINE_string("lr_scheduler_type", default="cosine", help="å­¦ä¹ ç‡è°ƒåº¦å™¨ç±»å‹")
flags.DEFINE_float("adam_b1", default=0.9, help="Adam b1")
flags.DEFINE_float("adam_b2", default=0.98, help="Adam b2")
flags.DEFINE_float("adam_eps", default=1e-9, help="Adam eps")

# åˆå§‹åŒ–å‚æ•° - ä½¿ç”¨transformer_lib_flaxä¸­çš„å®šä¹‰
# flags.DEFINE_string("kernel_init", default="uniform_scaling", help="æ ¸åˆå§‹åŒ–")
# flags.DEFINE_string("bias_init", default="uniform_scaling", help="åç½®åˆå§‹åŒ–")
# flags.DEFINE_string("linear_w_init", default="uniform_scaling", help="çº¿æ€§å±‚æƒé‡åˆå§‹åŒ–")
# flags.DEFINE_string("linear_bias_init", default="uniform_scaling", help="çº¿æ€§å±‚åç½®åˆå§‹åŒ–")
# flags.DEFINE_string("posemb_init", default="uniform_scaling", help="ä½ç½®ç¼–ç åˆå§‹åŒ–")
# flags.DEFINE_string("activation_fn", default="gelu", help="æ¿€æ´»å‡½æ•°")

FLAGS = flags.FLAGS


def train_step(state, seq, model, learning_rate_fn, dropout_rng=None):
    """æ‰§è¡Œå•æ­¥è®­ç»ƒ"""
    dropout_rng = random.fold_in(dropout_rng, state.step)

    def loss_fn(params):
        """è®­ç»ƒæŸå¤±å‡½æ•°"""
        output = model.apply({"params": params},
                           inputs=seq,
                           train=True,
                           rngs={"dropout": dropout_rng})
        return output[0].mean(), output

    lr = learning_rate_fn(state.step)
    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
    (_, extras), grads = grad_fn(state.params)
    grads = jax.lax.pmean(grads, "batch")
    new_state = state.apply_gradients(grads=grads)
    loss = jax.lax.pmean(extras[0], "batch")
    y_errors = jax.lax.psum(extras[1][0], "batch").sum(axis=0)
    metrics = {"loss": loss, "lr": lr, "y_errors": y_errors}
    return new_state, metrics


def get_model(rng, args):
    """åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨"""
    rng, init_rng = random.split(rng)

    # è®ºæ–‡æ ‡å‡†é…ç½®ï¼šL=16, H=512, M=4
    # å¦‚æœargsä¸­æ²¡æœ‰è®¾ç½®ï¼Œä½¿ç”¨è®ºæ–‡æ ‡å‡†å€¼
    n_layers = getattr(args, 'n_layers', 16)
    hidden_size = getattr(args, 'hidden_size', 512)
    n_heads = getattr(args, 'n_heads', 4)
    
    logging.info(f"Transformeré…ç½®: L={n_layers}, H={hidden_size}, M={n_heads}")

    # åˆ›å»ºTransformeré…ç½®
    config = transformer_lib_flax.TransformerConfig(
        num_heads=n_heads,
        num_layers=n_layers,
        hidden_size=hidden_size,
        loss_on_x_steps=args.loss_on_x_steps,
        norm_first=args.norm_first,
        disable_layer_norms=args.disable_layer_norms,
        final_layer_norm=args.final_layer_norm,
        kernel_init=transformer_lib_flax.nn_init_parser(args.kernel_init),
        bias_init=transformer_lib_flax.nn_init_parser(args.bias_init),
        linear_w_init=transformer_lib_flax.nn_init_parser(args.linear_w_init),
        linear_bias_init=transformer_lib_flax.nn_init_parser(args.linear_bias_init),
        posemb_init=transformer_lib_flax.nn_init_parser(args.posemb_init),
        max_len=(args.num_exemplars + 1) * 2,
        inner_dim=None,
        activation_fn=transformer_lib_flax.nn_activation_parser(args.activation_fn),
    )

    model = predictor_flax.CausalLM(config)

    @jax.jit
    def initialize_variables(init_rng):
        init_batch = jnp.ones((1, config.max_len, args.x_dim + 1), jnp.float32)
        init_variables = model.init(init_rng, inputs=init_batch, train=False)
        return init_variables

    init_variables = initialize_variables(init_rng)

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    if args.lr_scheduler_type == "cosine":
        scheduler = transformer_lib_flax.create_learning_rate_scheduler(
            base_learning_rate=args.learning_rate,
            num_warmup_steps=(args.n_epochs // 5) * args.n_iter_per_epoch,
            num_training_steps=args.n_epochs * args.n_iter_per_epoch,
        )
    elif args.lr_scheduler_type == "warmup":
        scheduler = transformer_lib_flax.create_learning_rate_scheduler_v2(
            factors="constant * linear_warmup",
            base_learning_rate=args.learning_rate,
            warmup_steps=(args.n_epochs // 5) * args.n_iter_per_epoch,
        )
    else:
        def scheduler(_):
            return args.learning_rate

    # åˆ›å»ºä¼˜åŒ–å™¨
    opt = optax.adamw(
        scheduler,
        b1=args.adam_b1,
        b2=args.adam_b2,
        eps=args.adam_eps,
        weight_decay=args.weight_decay,
    )

    # åˆ›å»ºè®­ç»ƒçŠ¶æ€
    state = train_state.TrainState.create(
        apply_fn=model.apply, 
        params=init_variables["params"], 
        tx=opt
    )

    # å¤åˆ¶åˆ°å¤šä¸ªè®¾å¤‡
    state = jax_utils.replicate(state)

    # åˆ›å»ºå¹¶è¡Œè®­ç»ƒæ­¥éª¤
    p_train_step = jax.pmap(
        lambda state, seq, dropout_rng: train_step(state, seq, model, scheduler, dropout_rng),
        axis_name="batch",
    )

    return model, state, p_train_step


def save_checkpoint(state, exp_folder):
    """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
    import numpy as np
    
    def get_array(x):
        try:
            return np.array(x)
        except:
            return None

    state = jax.tree_util.tree_map(get_array, jax_utils.unreplicate(state))
    ckpt_dir = os.path.abspath(os.path.join(exp_folder, "ckpt/"))
    gfile.makedirs(ckpt_dir)
    
    try:
        checkpoints.save_checkpoint(
            ckpt_dir=ckpt_dir, 
            target=state, 
            step=state.step, 
            overwrite=True
        )
        logging.info(f"æ¨¡å‹æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {ckpt_dir}")
    except Exception as e:
        logging.warning(f"ä¿å­˜æ£€æŸ¥ç‚¹æ—¶å‡ºç°é—®é¢˜: {e}")


def train_model(args):
    """è®­ç»ƒæ¨¡å‹çš„ä¸»å‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    utils.set_seed(args.seed)
    rng = random.PRNGKey(args.seed)
    rng, new_rng = random.split(rng)

    # åˆ›å»ºå®éªŒæ–‡ä»¶å¤¹
    gfile.makedirs(args.exp_folder)
    
    # ä¿å­˜é…ç½®
    with gfile.GFile(os.path.join(args.exp_folder, "config.json"), "w") as handle:
        json.dump(args.initial_dict, handle)

    logging.info("å¼€å§‹è®­ç»ƒ...")
    logging.info("="*70)
    logging.info("è®­ç»ƒé…ç½® (ä¸åŸå§‹é¡¹ç›®model_trainer.pyä¸€è‡´):")
    logging.info(f"  æ¨¡å‹: L=16, H=512, M=4")
    logging.info(f"  è®­ç»ƒ: {args.n_epochs * args.n_iter_per_epoch} iterations ({args.n_epochs} epochs Ã— {args.n_iter_per_epoch} iters)")
    logging.info(f"  æ•°æ®: {args.num_exemplars} (x,y)å¯¹, x_dim={args.x_dim}, batch={args.batch_size}")
    logging.info(f"  åˆ†å¸ƒ: p(w)=N(0,I), p(x)=N(0,I)")
    logging.info(f"  ä¼˜åŒ–: lr={args.learning_rate}, scheduler={args.lr_scheduler_type}, Adam(Î²1={args.adam_b1}, Î²2={args.adam_b2})")
    logging.info(f"  Warmup: {args.n_epochs // 5} epochs (~{(args.n_epochs // 5) * args.n_iter_per_epoch} steps, 20% of training)")
    logging.info("="*70)

    # åˆå§‹åŒ–æ¨¡å‹
    model, state, p_train_step = get_model(new_rng, args)

    # åˆ›å»ºæ•°æ®é‡‡æ ·å™¨
    rng, new_rng = random.split(rng)
    # Parse task probabilities
    task_probs = [args.prob0, args.prob1, args.prob2, args.prob3]
    prob_sum = sum(task_probs)
    
    # Check if using default values (all defaults would sum to 1.0)
    # Allow small numerical tolerance for floating point comparison
    if abs(prob_sum - 1.0) > 1e-6:
        raise ValueError(
            f"ä»»åŠ¡æ¦‚ç‡ä¹‹å’Œå¿…é¡»ç­‰äº1.0ï¼Œå½“å‰ä¸º {prob_sum}ã€‚\n"
            f"å½“å‰è®¾ç½®: prob0={args.prob0}, prob1={args.prob1}, prob2={args.prob2}, prob3={args.prob3}\n"
            f"è¯·è°ƒæ•´å‚æ•°ä½¿å…¶å’Œä¸º1.0"
        )
    
    logging.info(f"ğŸ“ ä»»åŠ¡æ¦‚ç‡è®¾ç½®: [Task1={args.prob0}, Task2={args.prob1}, Task3={args.prob2}, Task4={args.prob3}]")
    sampler = sampler_lib.Sampler(
        args.num_exemplars,
        args.x_dim,
        args.hidden_size,
        x_distribution_fn=sampler_lib.str_to_distribution_fn(args.x_distribution_str),
        w_distribution_fn=sampler_lib.str_to_distribution_fn(args.w_distribution_str),
        noise_std=args.noise_std,
        task_probs=task_probs,
    )

    # å‡†å¤‡dropoutéšæœºæ•°
    dropout_rngs = random.split(new_rng, jax.local_device_count())

    # è®­ç»ƒå¾ªç¯
    metrics_history = []
    
    for epoch in range(args.n_epochs):
        epoch_metrics = []
        
        for iteration in range(args.n_iter_per_epoch):
            # é‡‡æ ·æ•°æ®
            seqs, coefficients, *_ = sampler.sample(n=args.batch_size)
            seqs = jnp.array(seqs)
            coefficients = jnp.array(coefficients)
            seqs = common_utils.shard(seqs)
            
            # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
            state, metrics = p_train_step(state, seqs, dropout_rng=dropout_rngs)
            metrics = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], metrics))
            epoch_metrics.append(metrics)
            metrics_history.append(metrics)

        # è®¡ç®—å¹¶è®°å½•å¹³å‡æŒ‡æ ‡
        epoch_metrics = common_utils.stack_forest(epoch_metrics)
        avg_loss = jnp.mean(epoch_metrics["loss"])
        avg_lr = epoch_metrics["lr"][-1]
        y_errors = jnp.mean(epoch_metrics["y_errors"], axis=0) / args.batch_size
        
        logging.info(f"Epoch {epoch+1}/{args.n_epochs} - "
                    f"Loss: {avg_loss:.6f}, "
                    f"LR: {avg_lr:.2e}")
        
        # è¾“å‡ºä½ç½®lossæ•°ç»„ï¼ˆç®€æ´æ ¼å¼ï¼‰
        if len(y_errors) <= 40:
            loss_str = "[" + ", ".join([f"{float(y_errors[i]):.4f}" for i in range(len(y_errors))]) + "]"
            logging.info(f"Position losses: {loss_str}")

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % 100 == 0:
            save_checkpoint(state, args.exp_folder)

    # æœ€ç»ˆä¿å­˜
    save_checkpoint(state, args.exp_folder)
    
    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
    metrics_history = common_utils.stack_forest(metrics_history)
    metrics_history["y_errors"] = jnp.mean(metrics_history["y_errors"], axis=0) / args.batch_size
    
    with gfile.GFile(os.path.join(args.exp_folder, "metrics.pickle"), "wb") as handle:
        pickle.dump(metrics_history, handle)
    
    logging.info("è®­ç»ƒå®Œæˆï¼")
    logging.info(f"æœ€ç»ˆæŸå¤±: {jnp.mean(metrics_history['loss'][-100:]):.6f}")
    
    return state, metrics_history


def main(_):
    """ä¸»å‡½æ•°"""
    args = utils.flags_to_args()
    train_model(args)


if __name__ == "__main__":
    app.run(main)